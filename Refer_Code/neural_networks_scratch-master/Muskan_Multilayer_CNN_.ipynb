{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Muskan_Multilayer_CNN_",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZn1f75DoU6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8SR1VXoWPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolution(image, filt, bias, s=1):\n",
        "    (n_f, n_c_f, f, _) = filt.shape # filter dimensions\n",
        "    n_c, in_dim, _ = image.shape # image dimensions\n",
        "    \n",
        "    out_dim = int((in_dim - f)/s)+1 # calculate output dimensions\n",
        "    \n",
        "    assert n_c == n_c_f, \"Dimensions of filter must match dimensions of input image\"\n",
        "    \n",
        "    out = np.zeros((n_f,out_dim,out_dim))\n",
        "    \n",
        "    # convolve the filter over every part of the image, adding the bias at each step. \n",
        "    for curr_f in range(n_f):\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= in_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= in_dim:\n",
        "                out[curr_f, out_y, out_x] = np.sum(filt[curr_f] * image[:,curr_y:curr_y+f, curr_x:curr_x+f]) + bias[curr_f]\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "        \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNQPZnSQoeK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxpool(image, f=2, s=2):\n",
        "    '''\n",
        "    Downsample `image` using kernel size `f` and stride `s`\n",
        "    '''\n",
        "    n_c, h_prev, w_prev = image.shape\n",
        "    \n",
        "    h = int((h_prev - f)/s)+1\n",
        "    w = int((w_prev - f)/s)+1\n",
        "    \n",
        "    downsampled = np.zeros((n_c, h, w))\n",
        "    for i in range(n_c):\n",
        "        # slide maxpool window over each part of the image and assign the max value at each step to the output\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= h_prev:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= w_prev:\n",
        "                downsampled[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "    return downsampled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPZRUtABojMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    out = np.exp(X)\n",
        "    return out/np.sum(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpgGV0FJol9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categoricalCrossEntropy(probs, label):\n",
        "    return -np.sum(label * np.log(probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Drn--7Uoo4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
        "    '''\n",
        "    Backpropagation through a convolutional layer. \n",
        "    '''\n",
        "    (n_f, n_c, f, _) = filt.shape\n",
        "    (_, orig_dim, _) = conv_in.shape\n",
        "    ## initialize derivatives\n",
        "    dout = np.zeros(conv_in.shape) \n",
        "    dfilt = np.zeros(filt.shape)\n",
        "    dbias = np.zeros((n_f,1))\n",
        "    for curr_f in range(n_f):\n",
        "        # loop through all filters\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "                # loss gradient of filter (used to update the filter)\n",
        "                dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
        "                # loss gradient of the input to the convolution operation (conv1 in the case of this network)\n",
        "                dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f] \n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "        # loss gradient of the bias\n",
        "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
        "    \n",
        "    return dout, dfilt, dbias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHGZn6Hcosos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxpoolBackward(dpool, orig, f, s):\n",
        "    '''\n",
        "    Backpropagation through a maxpooling layer. The gradients are passed through the indices of greatest value in the original maxpooling during the forward step.\n",
        "    '''\n",
        "    (n_c, orig_dim, _) = orig.shape\n",
        "    \n",
        "    dout = np.zeros(orig.shape)\n",
        "    \n",
        "    for curr_c in range(n_c):\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "                # obtain index of largest value in input for current window\n",
        "                (a, b) = nanargmax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "                dout[curr_c, curr_y+a, curr_x+b] = dpool[curr_c, out_y, out_x]\n",
        "                \n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "        \n",
        "    return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is85kShOowMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv(image, label, params, conv_s, pool_f, pool_s):\n",
        "    \n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params \n",
        "    \n",
        "    ################################################\n",
        "    ############## Forward Operation ###############\n",
        "    ################################################\n",
        "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
        "    conv1[conv1<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
        "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
        "    \n",
        "    (nf2, dim2, _) = pooled.shape\n",
        "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
        "    \n",
        "    z = w3.dot(fc) + b3 # first dense layer\n",
        "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    out = w4.dot(z) + b4 # second dense layer\n",
        "     \n",
        "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
        "    \n",
        "    ################################################\n",
        "    #################### Loss ######################\n",
        "    ################################################\n",
        "    \n",
        "    loss = categoricalCrossEntropy(probs, label) # categorical cross-entropy loss\n",
        "        \n",
        "    ################################################\n",
        "    ############# Backward Operation ###############\n",
        "    ################################################\n",
        "    dout = probs - label # derivative of loss w.r.t. final dense layer output\n",
        "    dw4 = dout.dot(z.T) # loss gradient of final dense layer weights\n",
        "    db4 = np.sum(dout, axis = 1).reshape(b4.shape) # loss gradient of final dense layer biases\n",
        "    \n",
        "    dz = w4.T.dot(dout) # loss gradient of first dense layer outputs \n",
        "    dz[z<=0] = 0 # backpropagate through ReLU \n",
        "    dw3 = dz.dot(fc.T)\n",
        "    db3 = np.sum(dz, axis = 1).reshape(b3.shape)\n",
        "    \n",
        "    dfc = w3.T.dot(dz) # loss gradients of fully-connected layer (pooling layer)\n",
        "    dpool = dfc.reshape(pooled.shape) # reshape fully connected into dimensions of pooling layer\n",
        "    \n",
        "    dconv2 = maxpoolBackward(dpool, conv2, pool_f, pool_s) # backprop through the max-pooling layer(only neurons with highest activation in window get updated)\n",
        "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
        "    \n",
        "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) # backpropagate previous gradient through second convolutional layer.\n",
        "    dconv1[conv1<=0] = 0 # backpropagate through ReLU\n",
        "    \n",
        "    dimage, df1, db1 = convolutionBackward(dconv1, image, f1, conv_s) # backpropagate previous gradient through first convolutional layer.\n",
        "    \n",
        "    grads = [df1, df2, dw3, dw4, db1, db2, db3, db4] \n",
        "    \n",
        "    return grads, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxDkFFL1o2E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adamGD(batch, num_classes, lr, dim, n_c, beta1, beta2, params, cost):\n",
        "    '''\n",
        "    update the parameters through Adam gradient descnet.\n",
        "    '''\n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params\n",
        "    \n",
        "    X = batch[:,0:-1] # get batch inputs\n",
        "    X = X.reshape(len(batch), n_c, dim, dim)\n",
        "    Y = batch[:,-1] # get batch labels\n",
        "    \n",
        "    cost_ = 0\n",
        "    batch_size = len(batch)\n",
        "    \n",
        "    # initialize gradients and momentum,RMS params\n",
        "    df1 = np.zeros(f1.shape)\n",
        "    df2 = np.zeros(f2.shape)\n",
        "    dw3 = np.zeros(w3.shape)\n",
        "    dw4 = np.zeros(w4.shape)\n",
        "    db1 = np.zeros(b1.shape)\n",
        "    db2 = np.zeros(b2.shape)\n",
        "    db3 = np.zeros(b3.shape)\n",
        "    db4 = np.zeros(b4.shape)\n",
        "    \n",
        "    v1 = np.zeros(f1.shape)\n",
        "    v2 = np.zeros(f2.shape)\n",
        "    v3 = np.zeros(w3.shape)\n",
        "    v4 = np.zeros(w4.shape)\n",
        "    bv1 = np.zeros(b1.shape)\n",
        "    bv2 = np.zeros(b2.shape)\n",
        "    bv3 = np.zeros(b3.shape)\n",
        "    bv4 = np.zeros(b4.shape)\n",
        "    \n",
        "    s1 = np.zeros(f1.shape)\n",
        "    s2 = np.zeros(f2.shape)\n",
        "    s3 = np.zeros(w3.shape)\n",
        "    s4 = np.zeros(w4.shape)\n",
        "    bs1 = np.zeros(b1.shape)\n",
        "    bs2 = np.zeros(b2.shape)\n",
        "    bs3 = np.zeros(b3.shape)\n",
        "    bs4 = np.zeros(b4.shape)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        \n",
        "        x = X[i]\n",
        "        y = np.eye(num_classes)[int(Y[i])].reshape(num_classes, 1) # convert label to one-hot\n",
        "        \n",
        "        # Collect Gradients for training example\n",
        "        grads, loss = conv(x, y, params, 1, 2, 2)\n",
        "        [df1_, df2_, dw3_, dw4_, db1_, db2_, db3_, db4_] = grads\n",
        "        \n",
        "        df1+=df1_\n",
        "        db1+=db1_\n",
        "        df2+=df2_\n",
        "        db2+=db2_\n",
        "        dw3+=dw3_\n",
        "        db3+=db3_\n",
        "        dw4+=dw4_\n",
        "        db4+=db4_\n",
        "\n",
        "        cost_+= loss\n",
        "\n",
        "    # Parameter Update  \n",
        "        \n",
        "    v1 = beta1*v1 + (1-beta1)*df1/batch_size # momentum update\n",
        "    s1 = beta2*s1 + (1-beta2)*(df1/batch_size)**2 # RMSProp update\n",
        "    f1 -= lr * v1/np.sqrt(s1+1e-7) # combine momentum and RMSProp to perform update with Adam\n",
        "    \n",
        "    bv1 = beta1*bv1 + (1-beta1)*db1/batch_size\n",
        "    bs1 = beta2*bs1 + (1-beta2)*(db1/batch_size)**2\n",
        "    b1 -= lr * bv1/np.sqrt(bs1+1e-7)\n",
        "   \n",
        "    v2 = beta1*v2 + (1-beta1)*df2/batch_size\n",
        "    s2 = beta2*s2 + (1-beta2)*(df2/batch_size)**2\n",
        "    f2 -= lr * v2/np.sqrt(s2+1e-7)\n",
        "                       \n",
        "    bv2 = beta1*bv2 + (1-beta1) * db2/batch_size\n",
        "    bs2 = beta2*bs2 + (1-beta2)*(db2/batch_size)**2\n",
        "    b2 -= lr * bv2/np.sqrt(bs2+1e-7)\n",
        "    \n",
        "    v3 = beta1*v3 + (1-beta1) * dw3/batch_size\n",
        "    s3 = beta2*s3 + (1-beta2)*(dw3/batch_size)**2\n",
        "    w3 -= lr * v3/np.sqrt(s3+1e-7)\n",
        "    \n",
        "    bv3 = beta1*bv3 + (1-beta1) * db3/batch_size\n",
        "    bs3 = beta2*bs3 + (1-beta2)*(db3/batch_size)**2\n",
        "    b3 -= lr * bv3/np.sqrt(bs3+1e-7)\n",
        "    \n",
        "    v4 = beta1*v4 + (1-beta1) * dw4/batch_size\n",
        "    s4 = beta2*s4 + (1-beta2)*(dw4/batch_size)**2\n",
        "    w4 -= lr * v4 / np.sqrt(s4+1e-7)\n",
        "    \n",
        "    bv4 = beta1*bv4 + (1-beta1)*db4/batch_size\n",
        "    bs4 = beta2*bs4 + (1-beta2)*(db4/batch_size)**2\n",
        "    b4 -= lr * bv4 / np.sqrt(bs4+1e-7)\n",
        "    \n",
        "\n",
        "    cost_ = cost_/batch_size\n",
        "    cost.append(cost_)\n",
        "\n",
        "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
        "    \n",
        "    return params, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlbB4OSSpB0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_classes = 10, lr = 0.01, beta1 = 0.95, beta2 = 0.99, img_dim = 28, img_depth = 1, f = 5, num_filt1 = 8, num_filt2 = 8, batch_size = 32, num_epochs = 2, save_path = 'params.pkl'):\n",
        "\n",
        "    # training data\n",
        "    m =50000\n",
        "    X = extract_data('train-images-idx3-ubyte.gz', m, img_dim)\n",
        "    y_dash = extract_labels('train-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
        "    X-= int(np.mean(X))\n",
        "    X/= int(np.std(X))\n",
        "    train_data = np.hstack((X,y_dash))\n",
        "    \n",
        "    np.random.shuffle(train_data)\n",
        "\n",
        "    ## Initializing all the parameters\n",
        "    f1, f2, w3, w4 = (num_filt1 ,img_depth,f,f), (num_filt2 ,num_filt1,f,f), (128,800), (10, 128)\n",
        "    f1 = initializeFilter(f1)\n",
        "    f2 = initializeFilter(f2)\n",
        "    w3 = initializeWeight(w3)\n",
        "    w4 = initializeWeight(w4)\n",
        "\n",
        "    b1 = np.zeros((f1.shape[0],1))\n",
        "    b2 = np.zeros((f2.shape[0],1))\n",
        "    b3 = np.zeros((w3.shape[0],1))\n",
        "    b4 = np.zeros((w4.shape[0],1))\n",
        "\n",
        "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
        "\n",
        "    cost = []\n",
        "\n",
        "    print(\"LR:\"+str(lr)+\", Batch Size:\"+str(batch_size))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        np.random.shuffle(train_data)\n",
        "        batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
        "\n",
        "        t = tqdm(batches)\n",
        "        for x,batch in enumerate(t):\n",
        "            params, cost = adamGD(batch, num_classes, lr, img_dim, img_depth, beta1, beta2, params, cost)\n",
        "            t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
        "            \n",
        "    to_save = [params, cost]\n",
        "    \n",
        "    with open(save_path, 'wb') as file:\n",
        "        pickle.dump(to_save, file)\n",
        "        \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGpbiFGgpHfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(image, f1, f2, w3, w4, b1, b2, b3, b4, conv_s = 1, pool_f = 2, pool_s = 2):\n",
        "    '''\n",
        "    Make predictions with trained filters/weights. \n",
        "    '''\n",
        "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
        "    conv1[conv1<=0] = 0 #relu activation\n",
        "    \n",
        "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
        "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
        "    (nf2, dim2, _) = pooled.shape\n",
        "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
        "    \n",
        "    z = w3.dot(fc) + b3 # first dense layer\n",
        "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    out = w4.dot(z) + b4 # second dense layer\n",
        "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
        "    \n",
        "    return np.argmax(probs), np.max(probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHVVTr9mpLX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_data(filename, num_images, IMAGE_WIDTH):\n",
        "    '''\n",
        "    Extract images by reading the file bytestream. Reshape the read values into a 3D matrix of dimensions [m, h, w], where m \n",
        "    is the number of training examples.\n",
        "    '''\n",
        "    print('Extracting', filename)\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(16)\n",
        "        buf = bytestream.read(IMAGE_WIDTH * IMAGE_WIDTH * num_images)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        data = data.reshape(num_images, IMAGE_WIDTH*IMAGE_WIDTH)\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQV1gt7VpQJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_labels(filename, num_images):\n",
        "    '''\n",
        "    Extract label into vector of integer values of dimensions [m, 1], where m is the number of images.\n",
        "    '''\n",
        "    print('Extracting', filename)\n",
        "    with gzip.open(filename) as bytestream:\n",
        "        bytestream.read(8)\n",
        "        buf = bytestream.read(1 * num_images)\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mLO2AAypTyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initializeFilter(size, scale = 1.0):\n",
        "    stddev = scale/np.sqrt(np.prod(size))\n",
        "    return np.random.normal(loc = 0, scale = stddev, size = size)\n",
        "\n",
        "def initializeWeight(size):\n",
        "    return np.random.standard_normal(size=size) * 0.01\n",
        "\n",
        "def nanargmax(arr):\n",
        "    idx = np.nanargmax(arr)\n",
        "    idxs = np.unravel_index(idx, arr.shape)\n",
        "    return idxs  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhnEPxGYpcMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Train a convolutional neural network.')\n",
        "parser.add_argument('save_path', metavar = 'Save Path', help='name of file to save parameters in.')\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    save_path = args.save_path\n",
        "    \n",
        "    cost = train(save_path = save_path)\n",
        "\n",
        "    params, cost = pickle.load(open(save_path, 'rb'))\n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params\n",
        "    \n",
        "    # Plot cost \n",
        "    plt.plot(cost, 'r')\n",
        "    plt.xlabel('# Iterations')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.legend('Loss', loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "    # Get test data\n",
        "    m =10000\n",
        "    X = extract_data('t10k-images-idx3-ubyte.gz', m, 28)\n",
        "    y_dash = extract_labels('t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
        "    # Normalize the data\n",
        "    X-= int(np.mean(X)) # subtract mean\n",
        "    X/= int(np.std(X)) # divide by standard deviation\n",
        "    test_data = np.hstack((X,y_dash))\n",
        "    \n",
        "    X = test_data[:,0:-1]\n",
        "    X = X.reshape(len(test_data), 1, 28, 28)\n",
        "    y = test_data[:,-1]\n",
        "\n",
        "    corr = 0\n",
        "    digit_count = [0 for i in range(10)]\n",
        "    digit_correct = [0 for i in range(10)]\n",
        "   \n",
        "    print()\n",
        "    print(\"Computing accuracy over test set:\")\n",
        "\n",
        "    t = tqdm(range(len(X)), leave=True)\n",
        "\n",
        "    for i in t:\n",
        "        x = X[i]\n",
        "        pred, prob = predict(x, f1, f2, w3, w4, b1, b2, b3, b4)\n",
        "        digit_count[int(y[i])]+=1\n",
        "        if pred==y[i]:\n",
        "            corr+=1\n",
        "            digit_correct[pred]+=1\n",
        "\n",
        "        t.set_description(\"Acc:%0.2f%%\" % (float(corr/(i+1))*100))\n",
        "        \n",
        "    print(\"Overall Accuracy: %.2f\" % (float(corr/len(test_data)*100)))\n",
        "    x = np.arange(10)\n",
        "    digit_recall = [x/y for x,y in zip(digit_correct, digit_count)]\n",
        "    plt.xlabel('Digits')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.title(\"Recall on Test Set\")\n",
        "    plt.bar(x,digit_recall)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnKCgdj3p7sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}