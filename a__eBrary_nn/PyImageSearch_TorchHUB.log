
############ FOOBAR----Training an object detector from scratch in PyTorch

SOURCE -->> https://www.pyimagesearch.com/2021/11/01/training-an-object-detector-from-scratch-in-pytorch/


Torch Hub Series #1: Introduction to Torch Hub
SOURCE -->> https://www.pyimagesearch.com/2021/12/20/torch-hub-series-1-introduction-to-torch-hub/

https://www.pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/

https://www.pyimagesearch.com/2021/12/27/torch-hub-series-2-vgg-and-resnet/

https://www.pyimagesearch.com/2022/01/03/torch-hub-series-3-yolov5-and-ssd-models-on-object-detection/




Deep Learning PyTorch Tutorials
Torch Hub Series #1: Introduction to Torch Hub

by Devjyoti Chakraborty on December 20, 2021
Click here to download the source code to this post

In this tutorial, you will learn the basics of PyTorch’s Torch Hub.

This lesson is part 1 of a 6-part series on Torch Hub:

    Torch Hub Series #1: Introduction to Torch Hub (this tutorial)
    Torch Hub Series #2: VGG and ResNet
    Torch Hub Series #3: YOLO v5 and SSD — Models on Object Detection
    Torch Hub Series #4: PGAN — Model on GAN
    Torch Hub Series #5: MiDaS — Model on Depth Estimation
    Torch Hub Series #6: Image Segmentation

To learn how to use Torch Hub, just keep reading.

Looking for the source code to this post?
Jump Right To The Downloads Section

Introduction to Torch Hub

It was 2020 when my friends and I worked night and day to finish our final year project. Like most students in our year, we decided it was a good idea to leave it till the very end.

That wasn’t the brightest idea from our end. What followed were neverending nights of constant model calibration and training, burning through gigabytes of cloud storage, and maintaining records for deep learning model results.

The environment that we had created for ourselves did not only harm our efficiency, but it affected our morale. Due to the sheer individual brilliance of my other teammates, we managed to complete our project.

In retrospect, I realized how much more efficient our work could have been — and so much more enjoyable — had we chosen a better ecosystem to work in.

Fortunately, you don’t have to make the same mistakes I made.

The creators of PyTorch often emphasized that a key intention behind this initiative is to bridge the gap between research and production. PyTorch now stands toe to toe with its contemporaries on many fronts, being utilized equally in both research and production ecosystems.

One of the ways they’ve achieved this is through Torch Hub. Torch Hub as a concept was conceived to further extend PyTorch’s credibility as a production-based framework. In today’s tutorial, we’ll learn how to utilize Torch Hub to store and publish pre-trained models for wide-scale use.
What Is Torch Hub?

In Computer Science, many believe that a key puzzle piece in the bridge between research and production is reproducibility. Building on that very notion, PyTorch introduced Torch Hub, an Application Programmable Interface (API), which allows two programs to interact with each other and enhances the workflow for easy research reproducibility.

Torch Hub lets you publish pre-trained models to help in the cause of research sharing and reproducibility. The process of harnessing Torch Hub is simple, but before moving further, let’s configure the prerequisites of our system!
Configuring Your Development Environment

To follow this guide, you need to have the OpenCV library installed on your system.

Luckily, OpenCV is pip-installable:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
$ pip install opencv-contrib-python

If you need help configuring your development environment for OpenCV, I highly recommend that you read our pip install OpenCV guide — it will have you up and running in a matter of minutes.
Having Problems Configuring Your Development Environment?
Figure 1: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.

All that said, are you:

    Short on time?
    Learning on your employer’s administratively locked system?
    Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
    Ready to run the code right now on your Windows, macOS, or Linux system?

Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!
Project Structure

We first need to review our project structure.

Start by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.

Before moving to the directory, let’s take a look at the project structure in Figure 2.
Figure 2: Project Overview.

Today, we’ll be working with two directories. This is to help you better understand the use of Torch Hub.

The subdirectory is where we’ll initialize and train our model. Here, we’ll create a hubconf.py
script. The hubconf.py
script contains callable functions called entry_points
. These callable functions initialize and return the models which the user requires. Hence, this script will connect our own created model to Torch Hub.

In our main Project Directory, we’ll be using torch.hub.load
to load our model from Torch Hub. After loading the model with pre-trained weights, we’ll evaluate it on some sample data.
A Generalized Outlook on Torch Hub

Torch Hub already hosts an array of models for various tasks, as seen in Figure 3.
Figure 3: A glimpse of Torch Hub’s hosted Models.

As you can see, there are a total of 42 research models that Torch Hub has accepted in its official showcase. Each model belongs to one or more of the following labels: Audio, Generative, Natural Language Processing (NLP), scriptable, and vision. These models have also been trained on widely accepted benchmark datasets (e.g., Kinetics 400 and COCO 2017).

It’s easy to use these models in your projects using the torch.hub.load
function. Let’s look at an example of how it works.

We’ll look at Torch Hub’s official documentation of using a DCGAN trained on fashion-gen to generate some images.

(If you want to know more about DCGANs, do check out this blog.)
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# USAGE
# python inference.py
# import the necessary packages
import matplotlib.pyplot as plt
import torchvision
import argparse
import torch
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-n", "--num-images", type=int, default=64,
	help="# of images you want the DCGAN to generate")
args = vars(ap.parse_args())
# check if gpu is available for use
useGpu = True if torch.cuda.is_available() else False
# load the DCGAN model
model = torch.hub.load("facebookresearch/pytorch_GAN_zoo:hub", "DCGAN",
	pretrained=True, useGPU=useGpu)

On Lines 11-14, we created an argument parser to give the user more freedom to choose the batch size of generated images.

To use the Facebook Research pretrained DCGAN model, we just need the torch.hub.load
function as shown on Lines 20 and 21. The torch.hub.load
function here takes in the following arguments:

    repo_or_dir
    : The repository name in the format repo_owner/repo_name:branch/tag_name
    if the source
    argument is set to github
    . Otherwise, it will point to the desired path in your local machine.
    entry_point
    : To publish a model in torch hub, you need to have a script called hubconf.py
    in your repository/directory. In that script, you’ll define normal callable functions known as entry points. Calling the entry points to return the desired models. You’ll learn more about entry_point
    later in this blog.
    pretrained
    and useGpu
    : These fall under the *args
    or the arguments banner of this function. These arguments are for the callable model.

Now, this isn’t the only major function Torch Hub offers. You can use several other notable functions like torch.hub.list
to list all available entry points (callable functions) belonging to the repository, and torch.hub.help
to show the documentation docstring of the target entry point.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# generate random noise to input to the generator
(noise, _) = model.buildNoiseData(args["num_images"])
# turn off autograd and feed the input noise to the model
with torch.no_grad():
	generatedImages = model.test(noise)
# reconfigure the dimensions of the images to make them channel 
# last and display the output
output = torchvision.utils.make_grid(generatedImages).permute(
	1, 2, 0).cpu().numpy()
plt.imshow(output)
plt.show()

On Line 24, we use a function exclusive to the called model named buildNoiseData
to generate random input noise, keeping the input size in mind.

Turning off automatic gradients (Line 27), we generate images by feeding the noise to the model.

Before plotting the images, we do a dimensional re-shaping of the images on Lines 32-35 (since PyTorch works with channel first tensors, we need to make them channel last again). The output will look like Figure 4.
Figure 4: DCGAN output.

Voila! This is all you need to use a pre-trained state-of-the-art DCGAN model for your purposes. Using the pre-trained models in Torch Hub is THAT easy. However, we are not stopping there, are we?

Calling a pre-trained model to see how the latest state-of-the-art research performs is fine, but what about when we produce state-of-the-art results using our research? For that, we’ll next learn how to publish our own created models on Torch Hub.
Using Torch Hub on Your PyTorch Models

Let’s take a trip down memory lane to July 12, 2021, when Adrian Rosebrock released a blog post that taught you how to build a simple 2-layered neural network on PyTorch. The blog taught you to define your own simple neural networks, and train and test them on user-generated data.

Today, we’ll train our simple neural network and publish it using Torch Hub. I will not go into a full dissection of the code since a tutorial for that already exists. For a detailed and precise dive into building a simple neural network, refer to this blog.
Building a Simple Neural Network

Next, we’ll go through the salient parts of the code. For that, we’ll be moving into the subdirectory. First, let’s build our simple neural network in mlp.py
!
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# import the necessary packages
from collections import OrderedDict
import torch.nn as nn
# define the model function
def get_training_model(inFeatures=4, hiddenDim=8, nbClasses=3):
	# construct a shallow, sequential neural network
	mlpModel = nn.Sequential(OrderedDict([
		("hidden_layer_1", nn.Linear(inFeatures, hiddenDim)),
		("activation_1", nn.ReLU()),
		("output_layer", nn.Linear(hiddenDim, nbClasses))
	]))
	# return the sequential model
	return mlpModel

The get_training_model
function on Line 6 takes in parameters (input size, hidden layer size, output classes). Inside the function, we use nn.Sequential
to create a 2-layered neural network, consisting of a single hidden layer with ReLU activator and an output layer (Lines 8-12).
Training the Neural Network

We won’t be using any external dataset to train the model. Instead, we’ll generate data points ourselves. Let’s hop into train.py
.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# import the necessary packages
from pyimagesearch import mlp
from torch.optim import SGD
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_blobs
import torch.nn as nn
import torch
import os
# define the path to store your model weights
MODEL_PATH = os.path.join("output", "model_wt.pth")
# data generator function
def next_batch(inputs, targets, batchSize):
    # loop over the dataset
	for i in range(0, inputs.shape[0], batchSize):
		# yield a tuple of the current batched data and labels
		yield (inputs[i:i + batchSize], targets[i:i + batchSize])
# specify our batch size, number of epochs, and learning rate
BATCH_SIZE = 64
EPOCHS = 10
LR = 1e-2
# determine the device we will be using for training
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("[INFO] training using {}...".format(DEVICE))

First, we create a path to save the trained model weights on Line 11, which will be used later. The next_batch
function on Lines 14-18 will act as the data generator for our project, yielding batches of data for efficient training.

Next, we set up hyperparameters (Lines 21-23) and set our DEVICE
to cuda
if a compatible GPU is available (Line 26).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# generate a 3-class classification problem with 1000 data points,
# where each data point is a 4D feature vector
print("[INFO] preparing data...")
(X, y) = make_blobs(n_samples=1000, n_features=4, centers=3,
	cluster_std=2.5, random_state=95)
# create training and testing splits, and convert them to PyTorch
# tensors
(trainX, testX, trainY, testY) = train_test_split(X, y,
	test_size=0.15, random_state=95)
trainX = torch.from_numpy(trainX).float()
testX = torch.from_numpy(testX).float()
trainY = torch.from_numpy(trainY).float()
testY = torch.from_numpy(testY).float()

On Lines 32 and 33, we use the make_blobs
function to mimic data points of an actual three-class dataset. Using scikit-learn’s train_test_split
function, we create the training and test splits of the data.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# initialize our model and display its architecture
mlp = mlp.get_training_model().to(DEVICE)
print(mlp)
# initialize optimizer and loss function
opt = SGD(mlp.parameters(), lr=LR)
lossFunc = nn.CrossEntropyLoss()
# create a template to summarize current training progress
trainTemplate = "epoch: {} test loss: {:.3f} test accuracy: {:.3f}"

On Line 45, we call the get_training_model
function from the mlp.py
module and initialize the model.

We choose stochastic gradient descent as the optimizer (Line 49) and Cross-Entropy loss as the loss function (Line 50).

The trainTemplate
variable on Line 53 will act as a string template to print accuracy and loss.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# loop through the epochs
for epoch in range(0, EPOCHS):
	# initialize tracker variables and set our model to trainable
	print("[INFO] epoch: {}...".format(epoch + 1))
	trainLoss = 0
	trainAcc = 0
	samples = 0
	mlp.train()
	# loop over the current batch of data
	for (batchX, batchY) in next_batch(trainX, trainY, BATCH_SIZE):
		# flash data to the current device, run it through our
		# model, and calculate loss
		(batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))
		predictions = mlp(batchX)
		loss = lossFunc(predictions, batchY.long())
		# zero the gradients accumulated from the previous steps,
		# perform backpropagation, and update model parameters
		opt.zero_grad()
		loss.backward()
		opt.step()
		# update training loss, accuracy, and the number of samples
		# visited
		trainLoss += loss.item() * batchY.size(0)
		trainAcc += (predictions.max(1)[1] == batchY).sum().item()
		samples += batchY.size(0)
	# display model progress on the current training batch
	trainTemplate = "epoch: {} train loss: {:.3f} train accuracy: {:.3f}"
	print(trainTemplate.format(epoch + 1, (trainLoss / samples),
		(trainAcc / samples)))

Looping over the training epochs, we initialize the losses (Lines 59-61) and set the model to training mode (Line 62).

Using the next_batch
function, we iterate through batches of training data (Line 65). After loading them to the device (Line 68), the predictions for the data batch are obtained on Line 69. These predictions are then fed to the loss function for loss calculation (Line 70).

The gradients are flushed using zero_grad
(Line 74), followed by backpropagation on Line 75. Finally, the optimizer parameter is updated on Line 76.

For each epoch, the training loss, accuracy, and sample size variables are upgraded (Lines 80-82) and displayed using the template on Line 85.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
	# initialize tracker variables for testing, then set our model to
	# evaluation mode
	testLoss = 0
	testAcc = 0
	samples = 0
	mlp.eval()
	# initialize a no-gradient context
	with torch.no_grad():
		# loop over the current batch of test data
		for (batchX, batchY) in next_batch(testX, testY, BATCH_SIZE):
			# flash the data to the current device
			(batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))
			# run data through our model and calculate loss
			predictions = mlp(batchX)
			loss = lossFunc(predictions, batchY.long())
			# update test loss, accuracy, and the number of
			# samples visited
			testLoss += loss.item() * batchY.size(0)
			testAcc += (predictions.max(1)[1] == batchY).sum().item()
			samples += batchY.size(0)
		# display model progress on the current test batch
		testTemplate = "epoch: {} test loss: {:.3f} test accuracy: {:.3f}"
		print(testTemplate.format(epoch + 1, (testLoss / samples),
			(testAcc / samples)))
		print("")
# save model to the path for later use
torch.save(mlp.state_dict(), MODEL_PATH)

We set the model to eval
mode for model evaluation and do the same during the training phase, except for backpropagation.

On Line 121, we have the most important step of saving the model weights for later use.

Let’s assess the epoch-wise performance of our model!
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
[INFO] training using cpu...
[INFO] preparing data...
Sequential(
  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)
  (activation_1): ReLU()
  (output_layer): Linear(in_features=8, out_features=3, bias=True)
)
[INFO] epoch: 1...
epoch: 1 train loss: 0.798 train accuracy: 0.649
epoch: 1 test loss: 0.788 test accuracy: 0.613
[INFO] epoch: 2...
epoch: 2 train loss: 0.694 train accuracy: 0.665
epoch: 2 test loss: 0.717 test accuracy: 0.613
[INFO] epoch: 3...
epoch: 3 train loss: 0.635 train accuracy: 0.669
epoch: 3 test loss: 0.669 test accuracy: 0.613
...
[INFO] epoch: 7...
epoch: 7 train loss: 0.468 train accuracy: 0.693
epoch: 7 test loss: 0.457 test accuracy: 0.740
[INFO] epoch: 8...
epoch: 8 train loss: 0.385 train accuracy: 0.861
epoch: 8 test loss: 0.341 test accuracy: 0.973
[INFO] epoch: 9...
epoch: 9 train loss: 0.286 train accuracy: 0.980
epoch: 9 test loss: 0.237 test accuracy: 0.993
[INFO] epoch: 10...
epoch: 10 train loss: 0.211 train accuracy: 0.985
epoch: 10 test loss: 0.173 test accuracy: 0.993

Since we are training on data generated by paradigms we set, our training process went smoothly, reaching a final training accuracy of 0.985.
Configuring the hubconf.py script

With model training complete, our next step is to configure the hubconf.py
file in the repo to make our model accessible through Torch Hub.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# import the necessary packages
import torch
from pyimagesearch import mlp
# define entry point/callable function 
# to initialize and return model
def custom_model():
	""" # This docstring shows up in hub.help()
	Initializes the MLP model instance
	Loads weights from path and
	returns the model
	"""
	# initialize the model
	# load weights from path
	# returns model
	model = mlp.get_training_model()
	model.load_state_dict(torch.load("model_wt.pth"))
	return model

As mentioned earlier, we have created an entry point called custom_model
on Line 7. Inside the entry_point
, we initialize the simple neural network from the mlp.py
module (Line 16). Next, we load the weights we previously saved (Line 17). This current setup is made such that this function will look for the model weights in your project directory. You can host the weights on a cloud platform and choose the path accordingly.

Now, we’ll use Torch Hub to access this model and test it on our data.
Using torch.hub.load to Call Our Model

Coming back to our main project directory, let’s hop into the hub_usage.py
script.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# USAGE
# python hub_usage.py
# import the necessary packages
from pyimagesearch.data_gen import next_batch
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_blobs
import torch.nn as nn
import argparse
import torch
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-b", "--batch-size", type=int, default=64,
	help="input batch size")
args = vars(ap.parse_args())

After importing the necessary packages, we create an argument parser (Lines 13-16) for the user to input the batch size for the data.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# load the model using torch hub
print("[INFO] loading the model using torch hub...")
model = torch.hub.load("cr0wley-zz/torch_hub_test:main",
	"custom_model")
# generate a 3-class classification problem with 1000 data points,
# where each data point is a 4D feature vector
print("[INFO] preparing data...")
(X, Y) = make_blobs(n_samples=1000, n_features=4, centers=3,
	cluster_std=2.5, random_state=95)
# create training and testing splits, and convert them to PyTorch
# tensors
(trainX, testX, trainY, testY) = train_test_split(X, Y,
	test_size=0.15, random_state=95)
trainX = torch.from_numpy(trainX).float()
testX = torch.from_numpy(testX).float()
trainY = torch.from_numpy(trainY).float()
testY = torch.from_numpy(testY).float()

On Lines 20 and 21, we use torch.hub.load
to initialize our own model, the same way we had loaded the DCGAN model as shown earlier. The model has been initialized and the weights have been loaded according to the entry point in the hubconf.py
script in our subdirectory. As you can notice, we give the subdirectory github
repository as the parameter.

Now, for evaluation of the model, we’ll create data the same way we had created during our model training (Lines 26 and 27) and use train_test_split
to create data splits (Lines 31-36).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# initialize the neural network loss function
lossFunc = nn.CrossEntropyLoss()
# set device to cuda if available and initialize
# testing loss and accuracy
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
testLoss = 0
testAcc = 0
samples = 0
# set model to eval and grab a batch of data
print("[INFO] setting the model in evaluation mode...")
model.eval()
(batchX, batchY) = next(next_batch(testX, testY, args["batch_size"]))

On Line 39, we initialize the cross-entropy loss function as done during the model training. We proceed to initialize the evaluation metrics on Lines 44-46.

The model is set to evaluation mode (Line 50), and a single batch of data is grabbed to be evaluated upon by the model (Line 51).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
# initialize a no-gradient context
with torch.no_grad():
	# load the data into device
	(batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))
	# pass the data through the model to get the output and calculate
	# loss
	predictions = model(batchX)
	loss = lossFunc(predictions, batchY.long())
	# update test loss, accuracy, and the number of
	# samples visited
	testLoss += loss.item() * batchY.size(0)
	testAcc += (predictions.max(1)[1] == batchY).sum().item()
	samples += batchY.size(0)
	print("[INFO] test loss: {:.3f}".format(testLoss / samples))
	print("[INFO] test accuracy: {:.3f}".format(testAcc / samples))

Turning off the automatic gradients (Line 54), we load the batch of data to the device and feed it to the model (Lines 56-60). The lossFunc
proceeds to calculate the loss on Line 61.

With the help of the loss, we update the accuracy variable on Line 66, along with some other metrics like sample size (Line 67).

Let’s see how the model fared!
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #1: Introduction to Torch Hub
[INFO] loading the model using torch hub...
[INFO] preparing data...
[INFO] setting the model in evaluation mode...
Using cache found in /root/.cache/torch/hub/cr0wley-zz_torch_hub_test_main
[INFO] test loss: 0.086
[INFO] test accuracy: 0.969

Since we created our test data using the same paradigms used during training the model, it performed as expected, with a test accuracy of 0.969.

What's next? I recommend PyImageSearch University.
Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled

I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

    ✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
    ✓ 30+ Certificates of Completion
    ✓ 39h 44m on-demand video
    ✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
    ✓ Pre-configured Jupyter Notebooks in Google Colab
    ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
    ✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
    ✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
    ✓ Access on mobile, laptop, desktop, etc.

Click here to join PyImageSearch University

Summary

I cannot emphasize enough how important the reproduction of results is in today’s research world. Especially in machine learning, we’ve slowly reached a point where novel research ideas are getting more complex day by day. In a situation like that, researchers having a platform to easily make their research and results public takes a huge burden.

When you already have enough things to worry about as a researcher, having the tool to make your model and results public using a single script and a few lines of code is a great boon for us. Of course, as a project, Torch Hub will evolve more according to the user’s needs as days progress. Regardless of that, the ecosystem advocated by the creation of Torch Hub will help Machine Learning enthusiasts for generations to come.
Citation Information

Chakraborty, D. “Torch Hub Series #1: Introduction to Torch Hub,” PyImageSearch, 2021, https://www.pyimagesearch.com/2021/12/20/torch-hub-series-1-introduction-to-torch-hub/
@article{dev_2021_THS1,
   author = {Devjyoti Chakraborty},
   title = {{Torch Hub} Series \#1: Introduction to {Torch Hub}},
   journal = {PyImageSearch},
   year = {2021},
   note = {https://www.pyimagesearch.com/2021/12/20/torch-hub-series-1-introduction-to-torch-hub/},
}
Want free GPU credits to train models?

    We used Jarvislabs.ai, a GPU cloud, for all the experiments.
    We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit here.

In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.

To overcome this problem, we use GPUs. The problem is these GPUs are expensive and become outdated quickly.

GPUs are great because they take your Neural Network and train it quickly. The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally. Cloud GPUs let you use a GPU and only pay for the time you are running the GPU. It’s a brilliant idea that saves you money.

JarvisLabs provides the best-in-class GPUs, and PyImageSearch University students get between 10 - 50 hours on a world-class GPU (time depends on the specific GPU you select).

This gives you a chance to test-drive a monstrously powerful GPU on any of our tutorials in a jiffy. So join PyImageSearch University today and try for yourself.

Click here to get Jarvislabs credits now

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!

Download the Source Code and FREE 17-page Resource Guide

Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!

About the Author

Hey, I'm Devjyoti and I joined the ML bandwagon because it was too good to resist. Throughout my ML journey, I have been stuck many times while understanding concepts. I want to present those concepts to you in a way I wish they were presented to me, so the learning process becomes easier!
Reader Interactions

Previous Article:
GAN Training Challenges: DCGAN for Color Images

Next Article:
Torch Hub Series #2: VGG and ResNet
Comment section

Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.
Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
Join Now
Picked For You
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Torch Hub Series #4: PGAN — Model on GAN
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Torch Hub Series #2: VGG and ResNet
U-Net: Training Image Segmentation Models in PyTorch
Similar articles
Computer Graphics
Deep Learning
NeRF
Neural Radiance Fields (NeRF)
Tutorials
Computer Graphics and Deep Learning with NeRF using TensorFlow and Keras: Part 1
November 10, 2021

Getting Started
Image Processing
Tutorials
Bubble sheet multiple choice scanner and test grader using OMR, Python, and OpenCV
October 3, 2016

Kickstarter
PyImageSearch Gurus: The big list of computer vision topics you’ll master inside my course.
January 9, 2015

You can learn Computer Vision, Deep Learning, and OpenCV.

Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you’ll find our hand-picked tutorials, books, courses, and libraries to help you master CV and DL.
Footer
Topics

    Deep Learning
    Dlib Library
    Embedded/IoT and Computer Vision
    Face Applications
    Image Processing
    Interviews
    Keras

    Machine Learning and Computer Vision
    Medical Computer Vision
    Optical Character Recognition (OCR)
    Object Detection
    Object Tracking
    OpenCV Tutorials
    Raspberry Pi

Books & Courses

    FREE CV, DL, and OpenCV Crash Course
    Practical Python and OpenCV
    Deep Learning for Computer Vision with Python
    PyImageSearch Gurus Course
    Raspberry Pi for Computer Vision

PyImageSearch

    Get Started
    OpenCV Install Guides
    About
    FAQ
    Blog
    Contact
    Privacy Policy

© 2022 PyImageSearch. All Rights Reserved.









############ FOOBAR----Training an object detector from scratch in PyTorch

Deep Learning PyTorch Tutorials
Training an object detector from scratch in PyTorch

by Devjyoti Chakraborty on November 1, 2021
Click here to download the source code to this post

In this tutorial, you will learn how to train a custom object detector from scratch using PyTorch.

This lesson is part 2 of a 3-part series on advanced PyTorch techniques:

    Training a DCGAN in PyTorch (last week’s tutorial)
    Training an object detector from scratch in PyTorch (today’s tutorial)
    U-Net: Training Image Segmentation Models in PyTorch (next week’s blog post)

Since my childhood, the idea of artificial intelligence (AI) has fascinated me (like every other kid). But, of course, the concept of AI that I had was vastly different from what it actually was, unquestionably due to pop culture. Until the end of my teenage years, I firmly believed that the unchecked growth of AI would lead to something like the T-800 (the terminator from The Terminator). Fortunately, the actual scenario can be better explained using Figure 1:
Figure 1: Machine Learning.

Don’t get me wrong, though. Machine Learning may be a bunch of matrices and calculus coalesced together, but the sheer amount of things we can do with these can be best described by a single word: limitless.

One such application, which always intrigued me, was Object Detection. Pouring in image data to get labels was one thing, but making our model learn where the label is? That’s a whole different ball game, something right out of some espionage movie. And that is exactly what we’ll be going through today!

In today’s tutorial, we’ll learn how to train our very own object detector from scratch in PyTorch. This blog will help you:

    Understand the intuition behind Object Detection
    Understand the step-by-step approach to building your own Object Detector
    Learn how to fine-tune parameters to get ideal results

To learn how to train an object detector from scratch in Pytorch, just keep reading.
Training an Object Detector from scratch in PyTorch

Much before the power deep learning algorithms of today existed, Object Detection was a domain that was extensively worked on throughout history. From the late 1990s to the early 2020s, many new ideas were proposed, which are still used as benchmarks for deep learning algorithms to this day. Unfortunately, back then, researchers didn’t have much computation power at their disposal, so most of these techniques relied on lots of additional mathematics to reduce compute time. Thankfully, we wouldn’t be facing that problem.
Our Approach to Object Detection

Let’s first understand the intuition behind Object Detection. The approach we are going to take is quite similar to training a simple classifier. The weights of the classifier keep changing until it outputs the correct labels for a given dataset and reduces loss. We will be doing the exact same thing for today’s task, except our model will output 5 values, 4 of them being the coordinates of the bounding box surrounding our object. The 5th value is the label of the object being detected. Notice the architecture in Figure 2.
Figure 2: Schematic of Our Object Detector.

The main model will branch into two subsets: the regressor and the classifier. The former will output the bounding box’s starting and ending coordinates, while the latter will output the object label. The combined losses generated by these 5 values will serve in our backpropagation. Quite a simple way to start, isn’t it?

Of course, through the years, several powerful algorithms took over the Object Detection domain, like R-CNN and YOLO. But our approach will serve as a reasonable starting point to wrap your head around the basic idea behind Object Detection!
Configuring your development environment

To follow this guide, first and foremost, you need to have PyTorch installed in your system. To access PyTorch’s own set of models for vision computing, you will also need to have Torchvision in your system. For some array and storage operations, we have employed the use of numpy
. We are also using the imutils
package for data handling. For our plots, we will be using matplotlib
. For better tracking of our model training, we’ll be using tqdm
, and finally, we’ll be needing OpenCV in our system!

Luckily, all of the above-mentioned packages are pip-installable!
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
$ pip install opencv-contrib-python
$ pip install torch
$ pip install torchvision
$ pip install imutils
$ pip install matplotlib
$ pip install numpy
$ pip install tqdm

If you need help configuring your development environment for OpenCV, I highly recommend that you read my pip install OpenCV guide — it will have you up and running in a matter of minutes.
Having problems configuring your development environment?
Figure 3: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.

All that said, are you:

    Short on time?
    Learning on your employer’s administratively locked system?
    Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
    Ready to run the code right now on your Windows, macOS, or Linux system?

Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!
Project structure

We first need to review our project directory structure.

Start by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.

From there, take a look at the directory structure:
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
!tree .
.
├── dataset.zip
├── output
│   ├── detector.pth
│   ├── le.pickle
│   ├── plots
│   │   └── training.png
│   └── test_paths.txt
├── predict.py
├── pyimagesearch
│   ├── bbox_regressor.py
│   ├── config.py
│   ├── custom_tensor_dataset.py
│   └── __init__.py
└── train.py

The first item in the directory is dataset.zip
. This zip file contains the complete dataset (Images, labels, and bounding boxes). More about it in a later section.

Next, we have the output
directory. This directory is where all our saved models, results, and other important requirements are dumped.

There are two scripts in the parent directory:

    train.py
    : used to train our object detector
    predict.py
    : used to draw inference from our model and see the object detector in action 

Lastly, we have the most important directory, the pyimagesearch
directory. It houses 3 very important scripts.

    bbox_regressor.py
    : houses the complete object detector architecture
    config.py
    : contains the configuration of the end-to-end training and inference pipeline
    custom_tensor_dataset.py
    : contains a custom class for data preparation

That concludes the review of our project directory.
Configuring the prerequisites for Object Detection

Our first task is to configure several hyperparameters we’ll be using throughout the project. For that, let’s hop into the pyimagesearch
folder and open the config.py
script.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# import the necessary packages
import torch
import os
# define the base path to the input dataset and then use it to derive
# the path to the input images and annotation CSV files
BASE_PATH = "dataset"
IMAGES_PATH = os.path.sep.join([BASE_PATH, "images"])
ANNOTS_PATH = os.path.sep.join([BASE_PATH, "annotations"])
# define the path to the base output directory
BASE_OUTPUT = "output"
# define the path to the output model, label encoder, plots output
# directory, and testing image paths
MODEL_PATH = os.path.sep.join([BASE_OUTPUT, "detector.pth"])
LE_PATH = os.path.sep.join([BASE_OUTPUT, "le.pickle"])
PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, "plots"])
TEST_PATHS = os.path.sep.join([BASE_OUTPUT, "test_paths.txt"])

We start by defining several paths which we will later use. Then on Lines 7-12, we define paths for our datasets (Images and annotations) and output. Next, we create separate paths for our detector and label encoder, followed by paths for our plots and testing images (Lines 16-19).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# determine the current device and based on that set the pin memory
# flag
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PIN_MEMORY = True if DEVICE == "cuda" else False
# specify ImageNet mean and standard deviation
MEAN = [0.485, 0.456, 0.406]
STD = [0.229, 0.224, 0.225]
# initialize our initial learning rate, number of epochs to train
# for, and the batch size
INIT_LR = 1e-4
NUM_EPOCHS = 20
BATCH_SIZE = 32
# specify the loss weights
LABELS = 1.0
BBOX = 1.0

Since we are training an object detector, it’s advisable to train on a GPU instead of a CPU since the computations are more complex. Hence, we set our PyTorch device to CUDA if a CUDA-compatible GPU is available in our system (Lines 23 and 24).

We will, of course, be using PyTorch’s transforms during our dataset preparation. Hence we specify the mean and standard deviation values (Lines 27 and 28). The three values represent the channel-wise, width-wise, and height-wise mean and standard deviation, respectively. Finally, we initialize hyperparameters like learning rate, epochs, batch size, and Loss weights for our model (Lines 32-38).
Creating the Custom Object Detection Data processor

Let’s have a look at our data directory.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
!tree . 
.
├── dataset
│   ├── annotations
│   └── images
│       ├── airplane
│       ├── face
│       └── motorcycle

The dataset subdivides into two folders: annotations (which contains CSV files of bounding box start and end points) and images (which are further divided into three folders, each representing the classes we’ll be using today).

Since we’ll use PyTorch’s own DataLoader, it’s important to preprocess the data in a way that the DataLoader will accept. The custom_tensor_dataset.py
script will do exactly that.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# import the necessary packages
from torch.utils.data import Dataset
class CustomTensorDataset(Dataset):
	# initialize the constructor
	def __init__(self, tensors, transforms=None):
		self.tensors = tensors
		self.transforms = transforms

We have created a custom class, CustomTensorDataset
, which inherits from the torch.utils.data.Dataset
class (Line 4). This way, we can configure the internal functions to our needs while retaining the core properties of the torch.utils.data.Dataset
class.

On Lines 6-8, the constructor function __init__
is created. The constructor takes in two arguments:

    tensors
    : A tuple of three tensors, namely the image, label, and the bounding box coordinates.
    transforms
    : A torchvision.transforms
    instance which will be used to process the image. 

→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	def __getitem__(self, index):
		# grab the image, label, and its bounding box coordinates
		image = self.tensors[0][index]
		label = self.tensors[1][index]
		bbox = self.tensors[2][index]
		# transpose the image such that its channel dimension becomes
		# the leading one
		image = image.permute(2, 0, 1)
		# check to see if we have any image transformations to apply
		# and if so, apply them
		if self.transforms:
			image = self.transforms(image)
		# return a tuple of the images, labels, and bounding
		# box coordinates
		return (image, label, bbox)

Since we are using a custom class, we will override the parent (Dataset
) class’s methods. So, the __getitem__
method is altered according to our needs. But, first, the tensor
tuple is unpacked into its constituents (Lines 12-14).

The image tensor is originally in the form Height
× Width
× Channels
. However, all PyTorch models need their input to be “channel first.” Accordingly, the image.permute
method rearranges the image tensor (Line 18).

We add a check for the torchvision.transforms
instance on Lines 22 and 23. If the check yields true
, the image is passed through the transform
instance. After this, the __getitem__
method returns the image, label, and bounding boxes.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	def __len__(self):
		# return the size of the dataset
		return self.tensors[0].size(0)

The second method that we’ll override is the __len__
method. It returns the size of the image dataset tensor (Lines 29-31). This concludes the custom_tensor_dataset.py
script.
Building the Objection Detection Architecture

Coming to the model we’ll be needing for this project, we need to keep two things in mind. First, to avoid additional hassle and for efficient feature extraction, we’ll use a pre-trained model to act as the base model. Second, the base model will then be split into two parts; the box regressor and the label classifier. Both of these will be individual model entities.

The second thing to remember is that only the box regressor and the label classifier will have trainable weights. The weights of the pre-trained model will be left untouched, as shown in Figure 4.
Figure 4: Model Architecture.

With this in mind, let’s hop into bbox_regressor.py
!
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# import the necessary packages
from torch.nn import Dropout
from torch.nn import Identity
from torch.nn import Linear
from torch.nn import Module
from torch.nn import ReLU
from torch.nn import Sequential
from torch.nn import Sigmoid
class ObjectDetector(Module):
	def __init__(self, baseModel, numClasses):
		super(ObjectDetector, self).__init__()
		# initialize the base model and the number of classes
		self.baseModel = baseModel
		self.numClasses = numClasses

For the custom model ObjectDetector
, we’ll use torch.nn.Module
as the parent class (Line 10). For the constructor function __init__
, there are two external arguments; the base model and the number of labels (Lines 11-16).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
		# build the regressor head for outputting the bounding box
		# coordinates
		self.regressor = Sequential(
			Linear(baseModel.fc.in_features, 128),
			ReLU(),
			Linear(128, 64),
			ReLU(),
			Linear(64, 32),
			ReLU(),
			Linear(32, 4),
			Sigmoid()
		)

Moving on to the regressor, keep in mind that our end goal is to produce 4 separate values: the starting x-axis value, the starting y-axis value, the ending x-axis value, and the ending y-axis value. The first Linear
layer inputs the fully connected layer of the base model with an output size set to 128 (Line 21).

This is followed by a few Linear
and ReLU
layers (Lines 22-27), finally ending with a Linear
layer which outputs 4 values followed by a Sigmoid
layer (Line 28).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
		# build the classifier head to predict the class labels
		self.classifier = Sequential(
			Linear(baseModel.fc.in_features, 512),
			ReLU(),
			Dropout(),
			Linear(512, 512),
			ReLU(),
			Dropout(),
			Linear(512, self.numClasses)
		)
		# set the classifier of our base model to produce outputs
		# from the last convolution block
		self.baseModel.fc = Identity()

The next step is the classifier for the object label. In the Regressor, we take the base model’s fully connected layer’s feature size and plug it into the first Linear
layer (Line 33). This is followed by repeating the ReLU
, Dropout
, and Linear
layers (Lines 34-40). The Dropout
layers are generally used to help spread generalization and prevent overfitting.

The final step of the initialization is to make the base model’s fully connected layer into an Identity
layer, which means it’ll mirror the outputs produced by the convolution block right before it (Line 44).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
 	def forward(self, x):
		# pass the inputs through the base model and then obtain
		# predictions from two different branches of the network
		features = self.baseModel(x)
		bboxes = self.regressor(features)
		classLogits = self.classifier(features)
		# return the outputs as a tuple
		return (bboxes, classLogits)

Next comes the forward
step (Line 46). We simply take the output of the base model and pass it through the regressor and the classifier (Lines 49-51).

With that, we finish designing the architecture of our object detector.
Training the Object Detection Model

Just one more step remaining before we can see the object detector in action. So let’s hop over to the train.py
script and train the model!
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# USAGE
# python train.py
# import the necessary packages
from pyimagesearch.bbox_regressor import ObjectDetector
from pyimagesearch.custom_tensor_dataset import CustomTensorDataset
from pyimagesearch import config
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader
from torchvision import transforms
from torch.nn import CrossEntropyLoss
from torch.nn import MSELoss
from torch.optim import Adam
from torchvision.models import resnet50
from sklearn.model_selection import train_test_split
from imutils import paths
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import pickle
import torch
import time
import cv2
import os
# initialize the list of data (images), class labels, target bounding
# box coordinates, and image paths
print("[INFO] loading dataset...")
data = []
labels = []
bboxes = []
imagePaths = []

After importing the necessary packages, we create empty lists for our data, labels, bounding boxes, and image paths (Lines 29-32).

Now it’s time for some data pre-processing.
→ Launch Jupyter Notebook on Google Colab
 Training an Object Detector from scratch in PyTorch
# loop over all CSV files in the annotations directory
for csvPath in paths.list_files(config.ANNOTS_PATH, validExts=(".csv")):
	# load the contents of the current CSV annotations file
	rows = open(csvPath).read().strip().split("\n")
	# loop over the rows
	for row in rows:
		# break the row into the filename, bounding box coordinates,
		# and class label
		row = row.split(",")
		(filename, startX, startY, endX, endY, label) = row
		# derive the path to the input image, load the image (in
		# OpenCV format), and grab its dimensions
		imagePath = os.path.sep.join([config.IMAGES_PATH, label,
			filename])
		image = cv2.imread(imagePath)
		(h, w) = image.shape[:2]
		# scale the bounding box coordinates relative to the spatial
		# dimensions of the input image
		startX = float(startX) / w
		startY = float(startY) / h
		endX = float(endX) / w
		endY = float(endY) / h
		# load the image and preprocess it
		image = cv2.imread(imagePath)
		image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
		image = cv2.resize(image, (224, 224))
		# update our list of data, class labels, bounding boxes, and
		# image paths
		data.append(image)
		labels.append(label)
		bboxes.append((startX, startY, endX, endY))
		imagePaths.append(imagePath)

On Line 35, we start looping over all available CSVs in the directory. Opening the CSVs, we then begin looping over the rows to split the data (Lines 37-44).

After splitting the row values into a tuple of individual values, we first single out the image path (Line 48). Then, we use OpenCV to read the image and get its height and width (Lines 50 and 51).

The height and width values are then used to scale the bounding box coordinates to the range of 0
and 1
(Lines 55-58).

Next, we load the image and do some slight preprocessing (Lines 61-63).

The empty lists are then updated with the unpacked values, and the process repeats as each iteration passes (Lines 67-70).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# convert the data, class labels, bounding boxes, and image paths to
# NumPy arrays
data = np.array(data, dtype="float32")
labels = np.array(labels)
bboxes = np.array(bboxes, dtype="float32")
imagePaths = np.array(imagePaths)
# perform label encoding on the labels
le = LabelEncoder()
labels = le.fit_transform(labels)

For faster processing of data, the lists are converted into numpy
arrays (Lines 74-77). Since the labels are in string format, we use scikit-learn’s LabelEncoder
to transform them into their respective indices (Lines 80 and 81).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# partition the data into training and testing splits using 80% of
# the data for training and the remaining 20% for testing
split = train_test_split(data, labels, bboxes, imagePaths,
	test_size=0.20, random_state=42)
# unpack the data split
(trainImages, testImages) = split[:2]
(trainLabels, testLabels) = split[2:4]
(trainBBoxes, testBBoxes) = split[4:6]
(trainPaths, testPaths) = split[6:]

Using another handy scikit-learn tool called train_test_split
, we part the data into training and test sets, keeping an 80-20
ratio (Lines 85 and 86). Since the split will apply to all the arrays passed into the train_test_split
function, we can unpack them into tuples using simple row slicing (Lines 89-92).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# convert NumPy arrays to PyTorch tensors
(trainImages, testImages) = torch.tensor(trainImages),\
	torch.tensor(testImages)
(trainLabels, testLabels) = torch.tensor(trainLabels),\
	torch.tensor(testLabels)
(trainBBoxes, testBBoxes) = torch.tensor(trainBBoxes),\
	torch.tensor(testBBoxes)
# define normalization transforms
transforms = transforms.Compose([
	transforms.ToPILImage(),
	transforms.ToTensor(),
	transforms.Normalize(mean=config.MEAN, std=config.STD)
])

The unpacked train and test data, labels, and bounding boxes are then converted into PyTorch tensors from the numpy format (Lines 95-100). Next, we proceed to create a torchvision.transforms
instance to easily process the dataset (Lines 103-107). Through this, the dataset will also get normalized using the mean and standard deviation values defined in config.py
.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# convert NumPy arrays to PyTorch datasets
trainDS = CustomTensorDataset((trainImages, trainLabels, trainBBoxes),
	transforms=transforms)
testDS = CustomTensorDataset((testImages, testLabels, testBBoxes),
	transforms=transforms)
print("[INFO] total training samples: {}...".format(len(trainDS)))
print("[INFO] total test samples: {}...".format(len(testDS)))
# calculate steps per epoch for training and validation set
trainSteps = len(trainDS) // config.BATCH_SIZE
valSteps = len(testDS) // config.BATCH_SIZE
# create data loaders
trainLoader = DataLoader(trainDS, batch_size=config.BATCH_SIZE,
	shuffle=True, num_workers=os.cpu_count(), pin_memory=config.PIN_MEMORY)
testLoader = DataLoader(testDS, batch_size=config.BATCH_SIZE,
	num_workers=os.cpu_count(), pin_memory=config.PIN_MEMORY)

Remember, in the custom_tensor_dataset.py
script, we created a custom Dataset
class to cater to our exact needs. As of now, our required entities are just tensors. So, to turn them into a PyTorch DataLoader accepted format, we create training and testing instances of the CustomTensorDataset
class, passing the images, labels, and the bounding boxes as arguments (Lines 110-113).

On Lines 118 and 119, the steps per epoch values are calculated using the length of the datasets and the batch size value set in config.py
.

Finally, we pass the CustomTensorDataset
instances through the DataLoader
and create the train and test Data loaders (Lines 122-125).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# write the testing image paths to disk so that we can use then
# when evaluating/testing our object detector
print("[INFO] saving testing image paths...")
f = open(config.TEST_PATHS, "w")
f.write("\n".join(testPaths))
f.close()
# load the ResNet50 network
resnet = resnet50(pretrained=True)
# freeze all ResNet50 layers so they will *not* be updated during the
# training process
for param in resnet.parameters():
	param.requires_grad = False

Since we’ll be using the test image paths for evaluation later, it’s written to the disk (Lines 129-132).

For the base model in our architecture, we’ll be using a pre-trained resnet50 (Line 135). However, as mentioned before, the weights of the base model will be left untouched. Hence, we freeze the weights (Lines 139 and 140).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# create our custom object detector model and flash it to the current
# device
objectDetector = ObjectDetector(resnet, len(le.classes_))
objectDetector = objectDetector.to(config.DEVICE)
# define our loss functions
classLossFunc = CrossEntropyLoss()
bboxLossFunc = MSELoss()
# initialize the optimizer, compile the model, and show the model
# summary
opt = Adam(objectDetector.parameters(), lr=config.INIT_LR)
print(objectDetector)
# initialize a dictionary to store training history
H = {"total_train_loss": [], "total_val_loss": [], "train_class_acc": [],
	 "val_class_acc": []}

With the model prerequisites complete, we create our custom model instance and load it to the current device (Lines 144 and 145). For the classifier loss, Cross-Entropy loss is being used, while for the Box Regressor, we are sticking to Mean squared error loss (Lines 148 and 149). On Line 153, Adam
is set as the Object Detector optimizer. To track the training loss and other metrics, a dictionary H
is initialized on Lines 157 and 158.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# loop over epochs
print("[INFO] training the network...")
startTime = time.time()
for e in tqdm(range(config.NUM_EPOCHS)):
	# set the model in training mode
	objectDetector.train()
	# initialize the total training and validation loss
	totalTrainLoss = 0
	totalValLoss = 0
	# initialize the number of correct predictions in the training
	# and validation step
	trainCorrect = 0
	valCorrect = 0

For training speed assessment, the start time is noted (Line 162). Looping over the number of epochs, we first set the object detector to training mode (Line 165) and initialize the losses and number of correct predictions (Lines 168-174).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	# loop over the training set
	for (images, labels, bboxes) in trainLoader:
		# send the input to the device
		(images, labels, bboxes) = (images.to(config.DEVICE),
			labels.to(config.DEVICE), bboxes.to(config.DEVICE))
		# perform a forward pass and calculate the training loss
		predictions = objectDetector(images)
		bboxLoss = bboxLossFunc(predictions[0], bboxes)
		classLoss = classLossFunc(predictions[1], labels)
		totalLoss = (config.BBOX * bboxLoss) + (config.LABELS * classLoss)
		# zero out the gradients, perform the backpropagation step,
		# and update the weights
		opt.zero_grad()
		totalLoss.backward()
		opt.step()
		# add the loss to the total training loss so far and
		# calculate the number of correct predictions
		totalTrainLoss += totalLoss
		trainCorrect += (predictions[1].argmax(1) == labels).type(
			torch.float).sum().item()

Looping over the train data loader, we first load the images, labels, and bounding boxes to the device in use (Lines 179 and 180). Next, we plug the images into our Object Detector and store the predictions (Line 183). Finally, since the model will give two predictions (one for the label and one for the bounding box), we index those out and calculate those losses, respectively (Lines 183-185).

The combined value of both the losses will act as the total loss for the architecture. We multiply the respective loss weights for the bounding box loss and the label loss defined in config.py
to the losses and sum them up (Line 186).

With the help of PyTorch’s automatic gradient functionality, we simply reset the gradients, calculate the weights due to the loss generated, and update the parameter based on the gradient of the current step (Lines 190-192). It is important to reset the gradients because the backward
function keeps accumulating the gradients altogether. Since we only want the gradient pertaining to the current step, the opt.zero_grad
flushes out the previous values.

On Lines 196-198, we update the loss values and correct predictions.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	# switch off autograd
	with torch.no_grad():
		# set the model in evaluation mode
		objectDetector.eval()
		# loop over the validation set
		for (images, labels, bboxes) in testLoader:
			# send the input to the device
			(images, labels, bboxes) = (images.to(config.DEVICE),
				labels.to(config.DEVICE), bboxes.to(config.DEVICE))
			# make the predictions and calculate the validation loss
			predictions = objectDetector(images)
			bboxLoss = bboxLossFunc(predictions[0], bboxes)
			classLoss = classLossFunc(predictions[1], labels)
			totalLoss = (config.BBOX * bboxLoss) + \
				(config.LABELS * classLoss)
			totalValLoss += totalLoss
			# calculate the number of correct predictions
			valCorrect += (predictions[1].argmax(1) == labels).type(
				torch.float).sum().item()

Moving on to the model evaluation, we’ll first turn off automatic gradients and switch to the evaluation mode of the object detector (Lines 201-203). Then, looping over the test data, we’ll repeat the same process as done in training apart from updating the weights (Lines 212-214).

The combined loss is calculated in the same manner as the training step (Lines 215 and 216). Consequently, the total loss value and correct predictions are updated (Lines 217-221).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	# calculate the average training and validation loss
	avgTrainLoss = totalTrainLoss / trainSteps
	avgValLoss = totalValLoss / valSteps
	# calculate the training and validation accuracy
	trainCorrect = trainCorrect / len(trainDS)
	valCorrect = valCorrect / len(testDS)
	# update our training history
	H["total_train_loss"].append(avgTrainLoss.cpu().detach().numpy())
	H["train_class_acc"].append(trainCorrect)
	H["total_val_loss"].append(avgValLoss.cpu().detach().numpy())
	H["val_class_acc"].append(valCorrect)
	# print the model training and validation information
	print("[INFO] EPOCH: {}/{}".format(e + 1, config.NUM_EPOCHS))
	print("Train loss: {:.6f}, Train accuracy: {:.4f}".format(
		avgTrainLoss, trainCorrect))
	print("Val loss: {:.6f}, Val accuracy: {:.4f}".format(
		avgValLoss, valCorrect))
endTime = time.time()
print("[INFO] total time taken to train the model: {:.2f}s".format(
	endTime - startTime))

After one epoch, the average batchwise training and testing losses are calculated on Lines 224 and 225. We also calculate the training and testing accuracies of the epoch using the number of correct predictions (Lines 228 and 229).

Following the calculations, all values are logged in the model history dictionary H
(Lines 232-235), while the end time is calculated to see how long the training took and after exiting the loop (Line 243).
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# serialize the model to disk
print("[INFO] saving object detector model...")
torch.save(objectDetector, config.MODEL_PATH)
# serialize the label encoder to disk
print("[INFO] saving label encoder...")
f = open(config.LE_PATH, "wb")
f.write(pickle.dumps(le))
f.close()
# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()
plt.plot(H["total_train_loss"], label="total_train_loss")
plt.plot(H["total_val_loss"], label="total_val_loss")
plt.plot(H["train_class_acc"], label="train_class_acc")
plt.plot(H["val_class_acc"], label="val_class_acc")
plt.title("Total Training Loss and Classification Accuracy on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
# save the training plot
plotPath = os.path.sep.join([config.PLOTS_PATH, "training.png"])
plt.savefig(plotPath)

Since we’ll use the object detector for inference, we save it to the disk (Line 249). We also save the label encoder that was created so that the pattern remains unchanged (Lines 253-255)

To assess the model training, we plot all the metrics stored in the model history dictionary H
(Lines 258-271).

This ends the model training. Next, let’s look at how well the object detector trained!
Assessing the Object Detection Training

Since the bulk of the model will have its weights unchanged, the training shouldn’t take long. First, let’s take a look at some of the training epochs.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
[INFO] training the network...
  0%|          | 0/20 [00:00<?,  
  5%|▌         | 1/20 [00:16<05:08, 16.21s/it][INFO] EPOCH: 1/20
Train loss: 0.874699, Train accuracy: 0.7608
Val loss: 0.360270, Val accuracy: 0.9902
 10%|█         | 2/20 [00:31<04:46, 15.89s/it][INFO] EPOCH: 2/20
Train loss: 0.186642, Train accuracy: 0.9834
Val loss: 0.052412, Val accuracy: 1.0000
 15%|█▌        | 3/20 [00:47<04:28, 15.77s/it][INFO] EPOCH: 3/20
Train loss: 0.066982, Train accuracy: 0.9883
...
 85%|████████▌ | 17/20 [04:27<00:47, 15.73s/it][INFO] EPOCH: 17/20
Train loss: 0.011934, Train accuracy: 0.9975
Val loss: 0.004053, Val accuracy: 1.0000
 90%|█████████ | 18/20 [04:43<00:31, 15.67s/it][INFO] EPOCH: 18/20
Train loss: 0.009135, Train accuracy: 0.9975
Val loss: 0.003720, Val accuracy: 1.0000
 95%|█████████▌| 19/20 [04:58<00:15, 15.66s/it][INFO] EPOCH: 19/20
Train loss: 0.009403, Train accuracy: 0.9982
Val loss: 0.003248, Val accuracy: 1.0000
100%|██████████| 20/20 [05:14<00:00, 15.73s/it][INFO] EPOCH: 20/20
Train loss: 0.006543, Train accuracy: 0.9994
Val loss: 0.003041, Val accuracy: 1.0000
[INFO] total time taken to train the model: 314.68s

We see that the model reached astounding accuracies at 0.9994 and 1.0000 for training and validation, respectively. Let’s see the epoch-wise variation on the training plot Figure 5!
Figure 5: Training Plot.

The model reached saturation levels fairly quickly in both the training and validation values. Now it’s time to see the object detector in action!
Drawing Inference from the Object Detector

The final step in this journey is at the predict.py
script. Here, we will individually loop over the test images and draw bounding boxes with our predicted values.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# USAGE
# python predict.py --input dataset/images/face/image_0131.jpg
# import the necessary packages
from pyimagesearch import config
from torchvision import transforms
import mimetypes
import argparse
import imutils
import pickle
import torch
import cv2
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--input", required=True,
	help="path to input image/text file of image paths")
args = vars(ap.parse_args())

The argparse
module is used to write user-friendly command line interface commands. On Lines 15-18, we construct an argument parser to help the user select the test image.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# determine the input file type, but assume that we're working with
# single input image
filetype = mimetypes.guess_type(args["input"])[0]
imagePaths = [args["input"]]
# if the file type is a text file, then we need to process *multiple*
# images
if "text/plain" == filetype:
	# load the image paths in our testing file
	imagePaths = open(args["input"]).read().strip().split("\n")

We follow the argument parsing with steps to deal with any kind of input the user proceeds to give. On Lines 22 and 23, the imagePaths
variable is set to deal with a single input image, while on Lines 27-29, the event of multiple images is dealt with.
→ Launch Jupyter Notebook on Google Colab
Training and Object Detector from scratch in PyTorch
# load our object detector, set it evaluation mode, and label
# encoder from disk
print("[INFO] loading object detector...")
model = torch.load(config.MODEL_PATH).to(config.DEVICE)
model.eval()
le = pickle.loads(open(config.LE_PATH, "rb").read())
# define normalization transforms
transforms = transforms.Compose([
	transforms.ToPILImage(),
	transforms.ToTensor(),
	transforms.Normalize(mean=config.MEAN, std=config.STD)
])

The model which was trained using the train.py
script is called for evaluation (Lines 34 and 35). Similarly, the label encoder stored using the aforementioned script is loaded (Line 36). Since we’ll be needing to process the data again, another torchvision.transforms
instance is created, having the same arguments as the ones used during training.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
# loop over the images that we'll be testing using our bounding box
# regression model
for imagePath in imagePaths:
	# load the image, copy it, swap its colors channels, resize it, and
	# bring its channel dimension forward
	image = cv2.imread(imagePath)
	orig = image.copy()
	image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
	image = cv2.resize(image, (224, 224))
	image = image.transpose((2, 0, 1))
	# convert image to PyTorch tensor, normalize it, flash it to the
	# current device, and add a batch dimension
	image = torch.from_numpy(image)
	image = transforms(image).to(config.DEVICE)
	image = image.unsqueeze(0)

Looping over the test images, we read the image and apply some preprocessing to it (Lines 50-54). This is done since our image needs to be plugged into the object detector again.

We proceed to turn the image into a tensor, apply the torchvision.transforms
instance to it, and add a batching dimension to it (Lines 58-60). Our test image is now ready to be plugged into the object detector.
→ Launch Jupyter Notebook on Google Colab
Training an Object Detector from scratch in PyTorch
	# predict the bounding box of the object along with the class
	# label
	(boxPreds, labelPreds) = model(image)
	(startX, startY, endX, endY) = boxPreds[0]
	# determine the class label with the largest predicted
	# probability
	labelPreds = torch.nn.Softmax(dim=-1)(labelPreds)
	i = labelPreds.argmax(dim=-1).cpu()
	label = le.inverse_transform(i)[0]

First, the predictions from the model are obtained (Line 64). We proceed to unpack the bounding box values from the boxPreds
variable (Line 65).

A simple softmax function on the Label prediction will give us a better picture of the values corresponding to the classes. For that purpose, we use PyTorch’s own torch.nn.Softmax
on Line 69. Isolating the index with argmax
, we plug it in the Label encoder le
and use inverse_transform
(Index to value) to get the name of the label (Lines 69-71).
→ Launch Jupyter Notebook on Google Colab
Training an Ojbect Detector from scratch in PyTorch
	# resize the original image such that it fits on our screen, and
	# grab its dimensions
	orig = imutils.resize(orig, width=600)
	(h, w) = orig.shape[:2]
	# scale the predicted bounding box coordinates based on the image
	# dimensions
	startX = int(startX * w)
	startY = int(startY * h)
	endX = int(endX * w)
	endY = int(endY * h)
	# draw the predicted bounding box and class label on the image
	y = startY - 10 if startY - 10 > 10 else startY + 10
	cv2.putText(orig, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,
		0.65, (0, 255, 0), 2)
	cv2.rectangle(orig, (startX, startY), (endX, endY),
		(0, 255, 0), 2)
	# show the output image 
	cv2.imshow("Output", orig)
	cv2.waitKey(0)

On Line 75, we have resized the original image to fit our screen. The height and width of the resized image are then stored, to scale the predicted bounding box values based on the image (Lines 76-83). This is done because we had scaled down the annotations to the range 0
and 1
before fitting them to the model. Hence, all outputs would have to be scaled up for display purposes.

While displaying the bounding box, the label name will also be shown on top of it. For that purpose, we set up the y-axis value for our text on Line 86. Using OpenCV’s putText
function, we set up the label displayed on the image (Lines 87 and 88).

Finally, we use OpenCV’s rectangle
method to create the bounding box on the image (Lines 89 and 90). Since we have the starting x-axis, starting y-axis, ending x-axis, and ending y-axis values, it’s very easy to create a rectangle from them. This rectangle will surround our object.

This concludes our inference script. Let’s take a look at the results!
Object Detection in Action

Let’s see how our object detector fared, using one image from each class. We first use an image of an airplane (Figure 6), followed by an image under faces (Figure 7), and an image belonging to the motorcycle class (Figure 8).
Figure 6: An airplane, correctly predicted as an airplane.
Figure 7: A man, correctly categorized under face.
Figure 8: A Harley, correctly predicted as a motorcycle.

As it turns out, the accuracy values of our model weren’t lying. Not only did our model correctly guess the label, but the bounding boxes produced are also almost perfect!

With such precise detection and results, we can all agree that our little project was a success, can’t we?

What's next? I recommend PyImageSearch University.
Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled

I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

    ✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
    ✓ 30+ Certificates of Completion
    ✓ 39h 44m on-demand video
    ✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
    ✓ Pre-configured Jupyter Notebooks in Google Colab
    ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
    ✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
    ✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
    ✓ Access on mobile, laptop, desktop, etc.

Click here to join PyImageSearch University

Summary

While writing this Object Detection tutorial, I realized a few things in retrospect.

To be very honest, I had never liked using pre-trained models for my projects. It would feel that my work isn’t my work anymore. Obviously, that turned out to be a stupid notion, solidified with the fact that my first one-shot face classifier said that my best friend and I were the same people (believe me, we don’t look remotely similar).

I would say this tutorial served as a beautiful example of what happens when you have a well-trained feature extractor. Not only did we save time, but the end results were also brilliant. Take Figures 6 and 8 as examples. The predicted bounding boxes have minimal error.

Of course, this doesn’t mean there isn’t room for improvement. In Figure 7, the image has many elements, yet the object detector has managed to capture the general area of the object. However, it could have been more compact. We urge you to tinker around with the parameters to see if your results are better!

That being said, Object Detection really plays a vital role in our world today. Automated Traffic, face detections, self-driving cars are just a few of the real-world applications where Object Detection thrives. Each year, algorithms are designed to make the process faster and more compact. We have reached a stage where algorithms can concurrently detect all objects inside scenes of a video! I hope this tutorial has piqued your curiosity toward uncovering the intricacies of this domain.

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!

Download the Source Code and FREE 17-page Resource Guide

Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!

About the Author

Hey, I'm Devjyoti and I joined the ML bandwagon because it was too good to resist. Throughout my ML journey, I have been stuck many times while understanding concepts. I want to present those concepts to you in a way I wish they were presented to me, so the learning process becomes easier!
Reader Interactions

Previous Article:
Automatically OCR’ing Receipts and Scans

Next Article:
OCR’ing Business Cards
Comment section

Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.
Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
Join Now
Picked For You
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Torch Hub Series #4: PGAN — Model on GAN
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Torch Hub Series #2: VGG and ResNet
Torch Hub Series #1: Introduction to Torch Hub
Similar articles
Deep Learning
Keras and TensorFlow
Tutorials
Breaking captchas with deep learning, Keras, and TensorFlow
July 14, 2021

Image Processing
Tutorials
How-To: Python Compare Two Images
September 15, 2014

Deep Learning
Keras and TensorFlow
OpenCV Tutorials
Optical Character Recognition (OCR)
Tutorials
OpenCV Sudoku Solver and OCR
August 10, 2020

You can learn Computer Vision, Deep Learning, and OpenCV.

Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you’ll find our hand-picked tutorials, books, courses, and libraries to help you master CV and DL.
Footer
Topics

    Deep Learning
    Dlib Library
    Embedded/IoT and Computer Vision
    Face Applications
    Image Processing
    Interviews
    Keras

    Machine Learning and Computer Vision
    Medical Computer Vision
    Optical Character Recognition (OCR)
    Object Detection
    Object Tracking
    OpenCV Tutorials
    Raspberry Pi

Books & Courses

    FREE CV, DL, and OpenCV Crash Course
    Practical Python and OpenCV
    Deep Learning for Computer Vision with Python
    PyImageSearch Gurus Course
    Raspberry Pi for Computer Vision

PyImageSearch

    Get Started
    OpenCV Install Guides
    About
    FAQ
    Blog
    Contact
    Privacy Policy

© 2022 PyImageSearch. All Rights Reserved.


#### FOOBAR --- https://www.pyimagesearch.com/2022/01/03/torch-hub-series-3-yolov5-and-ssd-models-on-object-detection/
Source >> PyImageSearch -- Deep Learning PyTorch SSD Tutorials YOLOv5
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection

by Devjyoti Chakraborty on January 3, 2022
Click here to download the source code to this post

In my childhood, the movie Spy Kids was one of my favorite things to watch on television. Seeing kids of my age using futuristic gadgets to save the world and win the day might have been a common trope, but it still was fun to watch. Amongst things like Jetpacks and self-driving cars, my favorite was smart sunglasses, which could identify objects and people around you (it also doubled as a binocular), something like Figure 1.
Figure 1: Swiss Sunglasses (source).

Understandably, the conception of these gadgets in real life was hard to fathom at that time. However, now that we are in 2022, a self-driving car company (Tesla) is at the top of the motor industry, and detecting objects from real-time videos is a piece of cake!

So today, apart from understanding a fever dream of a young me, we will see how PyTorch Hub makes exploring these domains as easy.

In this tutorial, we will learn the intuition behind models like YOLOv5 and SSD300 and harness their powers using Torch Hub.

This lesson is part 3 of a 6-part series on Torch Hub:

    Torch Hub Series #1: Introduction to Torch Hub
    Torch Hub Series #2: VGG and ResNet
    Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection (this tutorial)
    Torch Hub Series #4: PGAN — Model on GAN
    Torch Hub Series #5: MiDaS — Model on Depth Estimation
    Torch Hub Series #6: Image Segmentation

To learn how to utilize YOLOv5 and SSD300, just keep reading.
Looking for the source code to this post?
Jump Right To The Downloads Section
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Object Detection at a Glance

Object Detection is undoubtedly a very alluring domain at first glance. Making a machine identify the exact position of an object inside an image makes me believe that we are another step closer to achieving the dream of mimicking the human brain. But even if we keep that aside, it has a variety of critical usage in today’s world. From Face Detection systems to helping Self-Driving Cars safely navigate, the list goes on. But how does it work exactly?

There are many methods to achieve Object Detection, using machine learning as the core idea. For example, in this blog post about training an object detector from scratch in PyTorch, we simply have an architecture that takes in the image as input and outputs 5 things; the class of the detected object and start and end values for the height and width of the object’s bounding box.

Essentially, we are grabbing annotated images and passing them through a simple CNN with an output of size 5. Consequently, like with every new thing developed in Machine Learning, more complex and intricate algorithms followed to improve it.

Note, if you think about the methodology I just mentioned in the previous paragraph, it might work just fine for images with a single object to detect. However, this will almost certainly hit a roadblock when multiple objects are inside a single image. So, to tackle this and other constraints like efficiency, we move on to YOLO(v1).

YOLO, or “You Only Look Once” (2015), introduced an ingenious way to tackle the shortcomings of a simple CNN detector. We split each image into an S×S grid, getting object positions corresponding to each cell. Of course, some cells won’t have any objects, while others will appear in multiple cells. Take a look at Figure 2.
Figure 2: YOLO image Grid (source).

It’s important to know the objects’ midpoint, height, and width for the complete image. Each cell will then output a probability value (probability of an object being in the cell), the detected object class, and the bounding box values unique to the cell.

Even if each cell can detect only one object, the existence of multiple cells nullifies the constraint. The result can be seen in Figure 3.
Figure 3: YOLO Process (source).

Despite great results, YOLOv1 had a major flaw; the closeness of objects inside an image often made the model miss some objects. Since its inception, several successors have been published, like YOLOv2, YOLOv3, and YOLOv4, each being better and faster than its predecessor. This brings us to one of today’s spotlights, YOLOv5.

Glenn Jocher, the creator of YOLOv5, decided against writing a paper and instead open sourced the model through GitHub. Initially, that raised a lot of concern since people thought the results weren’t reproducible. However, that notion was swiftly broken, and today, YOLOv5 is one of the official state-of-the-art models hosted in the Torch Hub showcase.

To understand what improvements YOLOv5 brought, we have to go back to YOLOv2. Amongst other things, YOLOv2 introduced the concept of anchor boxes. A series of predetermined bounding boxes, anchor boxes are of specific dimensions. These boxes are chosen depending on object sizes in your training datasets to capture the scale and aspect ratio of various object classes you want to detect. The network predicts the probabilities corresponding to the anchor boxes rather than the bounding boxes themselves.

But in practice, hoisted YOLO models are most often trained on the COCO dataset. This led to a problem since the custom dataset might not have the same anchor box definitions. YOLOv5 tackles this problem by introducing auto-learning anchor boxes. It also utilizes mosaic augmentation, mixing random images to make your model adept at identifying objects at a smaller scale.

The second item from today’s spotlight is the SSD or Single Shot MultiBox Detector model for object detection. The SSD300 originally used a VGG backbone for adept feature detection and utilized Szegedy‘s work on MultiBox, a method for quick class-agnostic bounding box coordinate recommendations, inspired SSD’s bounding box regression algorithm. Figure 4 shows the SSD architecture.
Figure 4: SSD Architecture (source).

Inspired by inception-net, the Multibox architecture created by Szegedy utilizes a multiscale convolutional architecture. Multibox uses a series of normal convolutional and 1×1
filters (changing channel size but keeping height and width intact) to incorporate a multiscale bounding box and confidence prediction model.

The SSD was famous for utilizing multiscale feature maps instead of single feature maps for detections. This allowed for finer detections and more granularity in the predictions. Using these feature maps, the anchor boxes for object predictions were generated.

It had outperformed its compatriots when it came out, especially in speed. Today we’ll be using Torch Hub’s showcased SSD, which uses a ResNet instead of a VGG net as its backbone. Also, some other changes, like removing some layers according to the Speed/accuracy trade-offs for modern convolutional object detectors paper, were applied to the model.

Today, we’ll learn how to harness the power of these models using Torch Hub and test them with our custom datasets!
Configuring Your Development Environment

To follow this guide, you need to have the OpenCV library installed on your system.

Luckily, OpenCV is pip-installable:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
$ pip install opencv-contrib-python

If you need help configuring your development environment for OpenCV, we highly recommend that you read our pip install OpenCV guide — it will have you up and running in a matter of minutes.
Having Problems Configuring Your Development Environment?
Figure 5: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.

All that said, are you:

    Short on time?
    Learning on your employer’s administratively locked system?
    Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
    Ready to run the code right now on your Windows, macOS, or Linux system?

Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!
Project Structure

We first need to review our project directory structure.

Start by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.

From there, take a look at the directory structure:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
!tree . 
.
├── output
│   ├── ssd_output
│   │   └── ssd_output.png
│   └── yolo_output
│       └── yolo_output.png
├── pyimagesearch
│   ├── config.py
│   ├── data_utils.py
├── ssd_inference.py
└── yolov5_inference.py

First, we have the output
directory, which will house the outputs we’ll get from each model.

In the pyimagesearch
directory, we have two scripts:

    config.py
    : This script houses the end to end configuration pipeline of the project
    data_utils.py
    : This script contains some helper functions for data processing

In the main directory, we have two scripts:

    ssd_inference.py
    : This script contains the SSD model inference for custom images.
    yolov5_inference.py
    : This script contains the YOLOv5 model inference for custom images. 

Downloading the Dataset

The first step is to configure our dataset according to our needs. Like in the previous tutorial, we’ll be using the Dogs & Cats Images dataset from Kaggle, owing to its relatively small size.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
$ mkdir ~/.kaggle
$ cp <path to your kaggle.json> ~/.kaggle/
$ chmod 600 ~/.kaggle/kaggle.json
$ kaggle datasets download -d chetankv/dogs-cats-images
$ unzip -qq dogs-cats-images.zip
$ rm -rf "/content/dog vs cat"

To use the dataset, you’ll need to have your own unique kaggle.json
file to connect to the Kaggle API (Line 2). The chmod 600
command on Line 3 gives the user full access to read and write files.

This is followed by the kaggle datasets download
command (Line 4) allows you to download any dataset hosted on their website. Finally, unzip the file and delete the unnecessary additions (Lines 5 and 6).

Let’s move on to the configuration pipeline.
Configuring the Prerequisites

Inside the pyimagesearch
directory, you’ll find a script called config.py
. This script will house the complete end-to-end configuration pipeline of our project.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# import the necessary packages
import torch
import os
# define the root directory followed by the test dataset paths
BASE_PATH = "dataset"
TEST_PATH = os.path.join(BASE_PATH, "test_set")
# specify image size and batch size
IMAGE_SIZE = 300
PRED_BATCH_SIZE = 4
# specify threshold confidence value for ssd detections
THRESHOLD = 0.50
# determine the device type 
DEVICE = torch.device("cuda") if torch.cuda.is_available() else "cpu"
# define paths to save output 
OUTPUT_PATH = "output"
SSD_OUTPUT = os.path.join(OUTPUT_PATH, "ssd_output")
YOLO_OUTPUT = os.path.join(OUTPUT_PATH, "yolo_output")

On Line 6, we have the BASE_PATH
variable, pointing to the dataset directory. Since we’ll only use the models to run inference, we’ll only need the test set (Line 7).

On Line 10, we have a variable named IMAGE_SIZE
, set to 300
. This is a requirement for the SSD Model since it is trained on size 300 x 300
images. The prediction batch size is set to 4
(Line 11), but readers are encouraged to play around with different sizes.

Next, we have a variable called THRESHOLD
, which will act as the confidence value threshold for the results of the SSD models, that is, only the results with more confidence value than the threshold will be kept (Line 14).

It’s advisable that you have a CUDA-compatible device for today’s project (Line 17), but since we’re not going for any heavy training, CPUs should work fine too.

Finally, we have created paths to save the outputs obtained from the model inferences (Lines 20-22).
Creating Helper Functions for Data Pipeline

Before we see the models in action, we have one more task remaining; Creating helper functions for data handling. For that, move to the data_utils.py
script located in the pyimagesearch
directory.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# import the necessary packages
from torch.utils.data import DataLoader
def get_dataloader(dataset, batchSize, shuffle=True):
	# create a dataloader and return it
	dataLoader= DataLoader(dataset, batch_size=batchSize,
		shuffle=shuffle)
	return dataLoader

The get_dataloader
(Line 4) function takes in the dataset, batch size, and shuffle arguments, returning a PyTorch Dataloader
(Lines 6 and 7) instance. The Dataloader
instance solves a lot of hassle, which goes into writing separate custom generator classes for huge datasets.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
def normalize(image, mean=128, std=128):
    # normalize the SSD input and return it 
    image = (image * 256 - mean) / std
    return image

The second function in the script, normalize
, is exclusively for the images we’ll be sending to the SSD model. It takes the image
, mean value, and standard deviation value as inputs, normalizes them, and returns the normalized image (Lines 10-13).
Testing Custom Images on YOLOv5

With the prerequisites taken care of, our next destination is the yolov5_inference.py
. We will prepare our custom data and feed it to the YOLO model.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# import necessary packages
from pyimagesearch.data_utils import get_dataloader
import pyimagesearch.config as config
from torchvision.transforms import Compose, ToTensor, Resize
from sklearn.model_selection import train_test_split
from torchvision.datasets import ImageFolder
from torch.utils.data import Subset
import matplotlib.pyplot as plt
import numpy as np
import random
import torch
import cv2
import os
# initialize test transform pipeline
testTransform = Compose([
	Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)), ToTensor()])
# create the test dataset
testDataset = ImageFolder(config.TEST_PATH, testTransform)
# initialize the test data loader
testLoader = get_dataloader(testDataset, config.PRED_BATCH_SIZE)

First, we create a PyTorch transform instance on Lines 16 and 17. Using another one of PyTorch’s stellar data utility functions called ImageFolder
, we can directly create a PyTorch Dataset instance (Line 20). However, for this function to work, we need to have the dataset in the same format as this project.

Once we have the dataset, we pass it through the get_dataloader
function created beforehand to get a generator like the PyTorch Dataloader instance (Line 23).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# initialize the yolov5 using torch hub
yoloModel = torch.hub.load("ultralytics/yolov5", "yolov5s")
# initialize iterable variable
sweeper = iter(testLoader)
# initialize image 
imageInput = []
# grab a batch of test data
print("[INFO] getting the test data...")
batch = next(sweeper)
(images, _) = (batch[0], batch[1])
# send the images to the device
images = images.to(config.DEVICE) 

On Line 26, the YOLOv5 is called using Torch Hub. Just to recap, the torch.hub.load
function takes the GitHub repository and the required entry point as its arguments. The entry point is the function’s name under which the model call is located in the hubconf.py
script of the desired repository.

The next step is very vital to our project. There are many ways we can grab random batches of images from a dataset. However, when we deal with progressively bigger datasets, relying on loops to get data will be less efficient.

Keeping that in mind, we will use a method that will be far more efficient than just loops. We’ll have the option of randomly grabbing data using the sweeper
iterable variable on Line 29. So each time you run the command on Line 36, you’ll have a different batch of data.

On Line 40, we load the grabbed data to the device we will use for computation.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# loop over all the batch 
for index in range(0, config.PRED_BATCH_SIZE):
	# grab each image
	# rearrange dimensions to channel last and
	# append them to image list
	image = images[index]
	image = image.permute((1, 2, 0))
	imageInput.append(image.cpu().detach().numpy()*255.0)
# pass the image list through the model
print("[INFO] getting detections from the test data...")
results = yoloModel(imageInput, size=300)

On Line 43, we have a loop in which we go over the grabbed images. Then, taking each image, we rearrange the dimensions to make them channel-last and append the result to our imageInput
list (Lines 47-49).

Next, we pass the list to the YOLOv5 model instance (Line 53).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# get random index value
randomIndex = random.randint(0,len(imageInput)-1)
# grab index result from results variable
imageIndex= results.pandas().xyxy[randomIndex]
# convert the bounding box values to integer
startX = int(imageIndex["xmin"][0])
startY = int(imageIndex["ymin"][0])
endX = int(imageIndex["xmax"][0])
endY = int(imageIndex["ymax"][0])
# draw the predicted bounding box and class label on the image
y = startY - 10 if startY - 10 > 10 else startY + 10
cv2.putText(imageInput[randomIndex], imageIndex["name"][0],
	(startX, y+10), cv2.FONT_HERSHEY_SIMPLEX,0.65, (0, 255, 0), 2)
cv2.rectangle(imageInput[randomIndex],
	(startX, startY), (endX, endY),(0, 255, 0), 2)
# check to see if the output directory already exists, if not
# make the output directory
if not os.path.exists(config.YOLO_OUTPUT):
    os.makedirs(config.YOLO_OUTPUT)
# show the output image and save it to path
plt.imshow(imageInput[randomIndex]/255.0)
# save plots to output directory
print("[INFO] saving the inference...")
outputFileName = os.path.join(config.YOLO_OUTPUT, "output.png")
plt.savefig(outputFileName)

The randomIndex
variable on Line 56 will act as the index of our choice while accessing the image that we’ll display. Using its value, the corresponding bounding box results are accessed on Line 59.

We split the specific values (starting X, starting Y, ending X, and ending Y coordinates) for the image on Lines 62-65. We have to use imageIndex["Column Name"][0]
format because results.pandas().xyxy[randomIndex]
returns a dataframe. Assuming there is one detection in the given image, we have to access its value by evoking the zeroth index of the required columns.

Using these values, we plot the label and bounding box on the image on Lines 69-72, using cv2.putText
and cv2.rectangle
, respectively. Given the coordinates, these functions will take the desired image and plot the required necessities.

Lastly, while plotting the image using plt.imshow
, we have to scale the values down (Line 80).

There you have YOLOv5’s results on your custom images! Let’s look at some results in Figures 6-8.
Figure 6: A cat, identified as a cat.
Figure 7: A dog, identified as a dog.
Figure 8: A dog identified as a dog.

As we can see from the results, the pretrained YOLOv5 model localizes reasonably well on all the images.
Testing Custom Images on the SSD Model

For inference on the SSD model, we’ll follow a pattern similar to what was done in the YOLOv5 inference script.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# import the necessary packages
from pyimagesearch.data_utils import get_dataloader
from pyimagesearch.data_utils import normalize
from pyimagesearch import config
from torchvision.datasets import ImageFolder
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split
from torchvision.transforms import Compose
from torchvision.transforms import ToTensor
from torchvision.transforms import Resize
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import numpy as np
import random
import torch
import cv2
import os
# initialize test transform pipeline
testTransform = Compose([
	Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)), ToTensor()])
# create the test dataset and initialize the test data loader
testDataset = ImageFolder(config.TEST_PATH, testTransform)
testLoader = get_dataloader(testDataset, config.PRED_BATCH_SIZE)
# initialize iterable variable
sweeper = iter(testLoader)
# list to store permuted images
imageInput = []

As earlier, we create the PyTorch Transform instance on Lines 20 and 21. Then, using the ImageFolder
utility function, we create the dataset instance as required, followed by the Dataloader
instance on Lines 24 and 25.

The iterable variable sweeper
is initialized on Line 28 for ease of access to the test data. Next, to store images that we will preprocess, we initialize a list called imageInput
(Line 31).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# grab a batch of test data
print("[INFO] getting the test data...")
batch = next(sweeper)
(images, _ ) = (batch[0], batch[1])
# switch off autograd
with torch.no_grad():
	# send the images to the device
	images = images.to(config.DEVICE) 
 
	# loop over all the batch 
	for index in range(0, config.PRED_BATCH_SIZE):
		# grab the image, de-normalize it, scale the raw pixel
		# intensities to the range [0, 255], and change the channel
		# ordering from channels first tp channels last
		image = images[index]
		image = image.permute((1, 2, 0))
		imageInput.append(image.cpu().detach().numpy())

The process shown in the above code block is again repeated from the YOLOv5 inference script. We grab a batch of data (Lines 35 and 36) and loop over them to rearrange each to channel-last and append them to our imageInput
list (Lines 39-50).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# call the required entry points
ssdModel = torch.hub.load("NVIDIA/DeepLearningExamples:torchhub",
	"nvidia_ssd")
utils = torch.hub.load("NVIDIA/DeepLearningExamples:torchhub", 
	"nvidia_ssd_processing_utils")
# flash model to the device and set it to eval mode
ssdModel.to(config.DEVICE)
ssdModel.eval()
# new list for processed input
processedInput = []
# loop over images and preprocess them
for image in imageInput:
	image = normalize (image)
	processedInput.append(image)
# convert the preprocessed images into tensors
inputTensor = utils.prepare_tensor(processedInput)

On Lines 53 and 54, we use the torch.hub.load
function to:

    Call the SSD model by evoking its corresponding repository and entry point name
    Call an additional Utility function to help preprocess the input images according to the SSD model’s need. 

The model is then loaded to our device in use and set to evaluation mode (Lines 59 and 60).

On Line 63, we create an empty list to keep the preprocessed input. Then, looping over the images, we normalize each of them and append them accordingly (Lines 66-68). Finally, to convert the preprocessed images to required tensors, we use a previously called utility function (Lines 71).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# turn off auto-grad
print("[INFO] getting detections from the test data...")
with torch.no_grad():
	# feed images to model
	detections = ssdModel(inputTensor)
# decode the results and filter them using the threshold
resultsPerInput = utils.decode_results(detections)
bestResults = [utils.pick_best(results,
	config.THRESHOLD) for results in resultsPerInput]

Switching off automatic gradients, feed the image tensors to the SSD model (Lines 75-77).

On Line 80, we use another function from utils
called decode_results
to get all the results corresponding to each input image. Now, since SSD gives you 8732 detections, we’ll use the threshold confidence value previously set in the config.py
script to keep only the ones with more than 50% confidence (Lines 81 and 82).

That would mean that the bestResults
list contains bounding box values, object classes, and confidence values corresponding to each image it encountered while it gave its detection outputs. This way, the index of this list will directly correspond to the index of our input list.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# get coco labels 
classesToLabels = utils.get_coco_object_dictionary()
# loop over the image batch
for image_idx in range(len(bestResults)):
	(fig, ax) = plt.subplots(1)
	# denormalize the image and plot the image
	image = processedInput[image_idx] / 2 + 0.5
	ax.imshow(image)
	# grab bbox, class, and confidence values
	(bboxes, classes, confidences) = bestResults[image_idx]

Since we don’t have a way to decode the class integer results to their corresponding labels, we’ll take the help of yet another function from utils
called get_coco_object_dictionary
(Line 85).

The next step is to match the results to their corresponding images and plot the bounding box on them. Accordingly, grab the corresponding image using the image index and denormalize it (Lines 88-93).

Using the same index, we grab the bounding box results, class name, and confidence values from the results (Line 96).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
	# loop over the detected bounding boxes
	for idx in range(len(bboxes)):
		# scale values up according to image size
		(left, bot, right, top) = bboxes[idx ] * 300
		# draw the bounding box on the image
		(x, y, w, h) = [val for val in [left, bot, right - left,
			top - bot]]
		rect = patches.Rectangle((x, y), w, h, linewidth=1,
			edgecolor="r", facecolor="none")
		ax.add_patch(rect)
		ax.text(x, y,
			"{} {:.0f}%".format(classesToLabels[classes[idx] - 1],
			confidences[idx] * 100),
			bbox=dict(facecolor="white", alpha=0.5))

Since there can be multiple detections for a single image, we create a loop and start iterating over the available bounding boxes (Line 99). The bounding box results are in the range of 0 and 1. So it’s important to scale them according to the image height and width when unpacking the bounding box values (Line 101).

Now, the SSD model outputs the left, bottom, right, and top coordinates instead of the YOLOv5s starting X, starting Y, ending X, and ending Y values. So, we have to calculate the starting X, starting Y, width, and height to plot the rectangle on the image (Lines 104-107).

Finally, we add the object class name with the help of the classesToLabels
function on Lines 109-112.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #3: YOLOv5 and SSD
# check to see if the output directory already exists, if not
# make the output directory
if not os.path.exists(config.SSD_OUTPUT):
    os.makedirs(config.SSD_OUTPUT)
# save plots to output directory
print("[INFO] saving the inference...")
outputFileName = os.path.join(config.SSD_OUTPUT, "output.png")
plt.savefig(outputFileName)

We end the script with our image plotted by saving the output image file to our previously set location (Lines 121 and 120).

Let’s see the script in action!

In Figures 9-11, we have the bounding boxes predicted by the SSD model on images from our custom dataset.
Figure 9: A cat, along with its predicted confidence values.
Figure 10: A cat, along with its predicted confidence values.
Figure 11: Two dogs, along with their predicted confidence values.

It’s commendable how in Figure 11, the SSD model managed to figure out the puppy, which has almost camouflaged itself with its parent. Otherwise, the SSD model performed reasonably well on most images, with the confidence values telling us how sure it is of its predictions.
What's next? I recommend PyImageSearch University.
Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled

I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

    ✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
    ✓ 30+ Certificates of Completion
    ✓ 39h 44m on-demand video
    ✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
    ✓ Pre-configured Jupyter Notebooks in Google Colab
    ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
    ✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
    ✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
    ✓ Access on mobile, laptop, desktop, etc.

Click here to join PyImageSearch University
Summary

Since Object Detection has become a major part of our lives, having access to models that can replicate high-level research/industry-level results is a huge boon for the learning crowd.

Torch Hub’s simple yet stellar system of entry point calling is again displayed in this tutorial, where we can call the pretrained all-powerful models and their auxiliary helper functions to help us pre-process our data better. The beauty of this whole process is that if the model owners decide to push a change to their repository, instead of going through many processes to change hosted data, they need to push their changes into their repository itself.

With the whole process of dealing with hosted data simplified, PyTorch has hit a home run with their whole process of collaboration with GitHub. This way, even us users who call the models can learn more about it in the repositories themselves since it would have to be public.

I hope this tutorial served as a good starting point on using these models for your custom tasks. Readers are instructed to try out their images or think about using these models for their custom tasks!
Citation Information

Chakraborty, D. “Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection,” PyImageSearch, 2022, https://www.pyimagesearch.com/2022/01/03/torch-hub-series-3-yolov5-ssd-models-on-object-detection/
@article{dev_2022_THS3,
  author = {Devjyoti Chakraborty},
  title = {{Torch Hub} Series \#3: {YOLOv5} and {SSD} — Models on Object Detection},
  journal = {PyImageSearch},
  year = {2022},
  note = {https://www.pyimagesearch.com/2022/01/03/torch-hub-series-3-yolov5-and-ssd-models-on-object-detection/},
}
Want free GPU credits to train models?

    We used Jarvislabs.ai, a GPU cloud, for all the experiments.
    We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit here.

In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.

To overcome this problem, we use GPUs. The problem is these GPUs are expensive and become outdated quickly.

GPUs are great because they take your Neural Network and train it quickly. The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally. Cloud GPUs let you use a GPU and only pay for the time you are running the GPU. It’s a brilliant idea that saves you money.

JarvisLabs provides the best-in-class GPUs, and PyImageSearch University students get between 10 - 50 hours on a world-class GPU (time depends on the specific GPU you select).

This gives you a chance to test-drive a monstrously powerful GPU on any of our tutorials in a jiffy. So join PyImageSearch University today and try for yourself.

Click here to get Jarvislabs credits now

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!
Download the Source Code and FREE 17-page Resource Guide

Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!
About the Author

Hey, I'm Devjyoti and I joined the ML bandwagon because it was too good to resist. Throughout my ML journey, I have been stuck many times while understanding concepts. I want to present those concepts to you in a way I wish they were presented to me, so the learning process becomes easier!
Reader Interactions

Previous Article:
Torch Hub Series #2: VGG and ResNet

Next Article:
Torch Hub Series #4: PGAN — Model on GAN
Comment section

Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.
Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
Join Now
Picked For You
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Torch Hub Series #4: PGAN — Model on GAN
Torch Hub Series #2: VGG and ResNet
Torch Hub Series #1: Introduction to Torch Hub
U-Net: Training Image Segmentation Models in PyTorch
Similar articles
Deep Learning
Face Applications
Tutorials
Liveness Detection with OpenCV
March 11, 2019

Image Processing, Tutorials
OpenCV Tutorials
OpenCV Edge Detection ( cv2.Canny )
May 12, 2021

Image Descriptors
Tutorials
Implementing RootSIFT in Python and OpenCV
April 13, 2015

You can learn Computer Vision, Deep Learning, and OpenCV.

Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you’ll find our hand-picked tutorials, books, courses, and libraries to help you master CV and DL.
Footer
Topics

    Deep Learning
    Dlib Library
    Embedded/IoT and Computer Vision
    Face Applications
    Image Processing
    Interviews
    Keras

    Machine Learning and Computer Vision
    Medical Computer Vision
    Optical Character Recognition (OCR)
    Object Detection
    Object Tracking
    OpenCV Tutorials
    Raspberry Pi

Books & Courses

    FREE CV, DL, and OpenCV Crash Course
    Practical Python and OpenCV
    Deep Learning for Computer Vision with Python
    PyImageSearch Gurus Course
    Raspberry Pi for Computer Vision

PyImageSearch

    Get Started
    OpenCV Install Guides
    About
    FAQ
    Blog
    Contact
    Privacy Policy

© 2022 PyImageSearch. All Rights Reserved.




############ FOOBAR__VGG_and_ResNet__PyImageSearch>> Deep Learning PyTorch ResNet Tutorials VGG
SOURCE >> https://www.pyimagesearch.com/2021/12/27/torch-hub-series-2-vgg-and-resnet/
Deep Learning PyTorch ResNet Tutorials VGG
Torch Hub Series #2: VGG and ResNet

by Devjyoti Chakraborty on December 27, 2021
Click here to download the source code to this post

In the previous tutorial, we learned the essence behind Torch Hub and its conception. Then, we published our model using the intricacies of Torch Hub and accessed it through the same. But, what happens when our work requires us to utilize one of the many all-powerful models available on Torch Hub?

In this tutorial, we’ll learn how to harness the power of the most common models called using Torch Hub: the VGG and ResNet family of models. We’ll learn the core ideas behind these models and fine-tune them for a task of our choice.

This lesson is part 2 of a 6-part series on Torch Hub:

    Torch Hub Series #1: Introduction to Torch Hub
    Torch Hub Series #2: VGG and ResNet (this tutorial)
    Torch Hub Series #3: YOLO v5 and SSD — Models on Object Detection
    Torch Hub Series #4: PGAN — Model on GAN
    Torch Hub Series #5: MiDaS — Model on Depth Estimation
    Torch Hub Series #6: Image Segmentation

To learn how to harness the power of VGG nets and ResNets using Torch Hub, just keep reading.
Looking for the source code to this post?
Jump Right To The Downloads Section
Torch Hub Series #2: VGG and ResNet
VGG and ResNets

Let’s be honest, in every deep learning enthusiast’s life, Transfer learning will always play a huge role. We don’t always possess the necessary hardware to train models from scratch, especially on gigabytes of data. Cloud environments do make our lives easier for us, but they are clearly limited in usage.

Now, you might wonder if we must try out everything we learn in our machine learning journey. This can be best explained using Figure 1.
Figure 1: Equal parts theory and practice.

Theory and practice are needed in equal parts when you are in the Machine Learning domain. Going by this notion, hardware limitations would seriously impact your journey in Machine Learning. Thankfully, the good samaritans of the machine learning community help us bypass these problems by uploading pre-trained model weights on the internet. These models are trained on huge datasets, making them extremely capable feature extractors.

Not only can you use these models for your tasks, but these are also used as benchmarks. Now, you must be wondering if a model trained on a particular dataset will work for a task unique to your problem.

That’s a very legitimate question. But think about the complete scenario for a second. For example, suppose you have a model trained on ImageNet (14 Million images and 20,000 classes). In that case, fine-tuning it for a similar and more specific image classification will give you good results since your model is already an adept feature extractor. Since our task today will be to fine-tune a VGG/ResNet model, we will see how adept our model is from the first epoch!

With an abundance of pre-trained model weights available on the web, Torch Hub identifies all the possible problems that might pop up and solves them by condensing this whole process into a single line. As a result, not only can you load SOTA models in your local system, but you can also choose if you want them pre-trained or not.

Without further ado, let’s move on to the prerequisites for this tutorial.
Configuring Your Development Environment

To follow this guide, you need to have the PyTorch framework installed on your system.

Luckily, it is pip-installable:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
$ pip install pytorch

If you need help configuring your development environment for OpenCV, we highly recommend that you read our pip install OpenCV guide — it will have you up and running in a matter of minutes.
Having Problems Configuring Your Development Environment?
Figure 2: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.

All that said, are you:

    Short on time?
    Learning on your employer’s administratively locked system?
    Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
    Ready to run the code right now on your Windows, macOS, or Linux system?

Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!
Project Structure

We first need to review our project directory structure.

Start by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.

From there, take a look at the directory structure:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
$ tree .
.
├── inference.py
├── pyimagesearch
│   ├── classifier.py
│   ├── config.py
│   └── datautils.py
└── train.py
1 directory, 5 files

Inside the pyimagesearch
, we have 3 scripts:

    classifier.py
    : Houses the model architecture for the project
    config.py
    : Contains an end to end configuration pipeline for the project
    datautils.py
    : Houses the two data utility functions we’ll be using in the project

In the parent directory, we have two scripts:

    inference.py
    : To infer from our trained model weights
    train.py
    : To train the model on the desired dataset

A Recap of the VGG and ResNet Architectures

The VGG16 architecture was introduced in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition.” It borrows its core idea from AlexNet while replacing the large-sized convolution filters with multiple 3×3
convolutional filters. In Figure 3, we can see the complete architecture.
Figure 3: VGG16 Architecture (source).

The small filter size of the plethora of convolution filters and the deep architecture of the network outperformed lots of benchmark models of that time. As a result, to this day, VGG16 is considered to be a state-of-the-art model for the ImageNet dataset.

Unfortunately, VGG16 had some major flaws. First, due to the nature of the network, it had several weight parameters. This not only made the models heavier but also increased the inference time for these models.

Understanding the limitations posed by VGG nets, we move on to their spiritual successors; ResNets. Introduced by Kaiming He and Jian Sun, the sheer genius behind the idea of ResNets not only outperformed the VGG nets in many instances, but their architecture enabled a faster inference time as well.

The main idea behind ResNets can be seen in Figure 4.
Figure 4: Residual Block Architecture (source).

This architecture is termed as “Residual Blocks.” As you can see, the output of a layer not only gets fed to the next layer but also makes a hop and is fed to another layer down the architecture.

Now, this idea right away eliminates the chances of vanishing gradients. But the main idea here is that information from the previous layers is kept alive in the later layers. Hence, an elaborate array of feature maps play a role in adaptively deciding the output of these residual block layers.

ResNet turned out to be a giant leap for the machine learning community. Not only did the results outperform lots of deep architectures at the time of its conception, but ResNet also introduced a whole new direction of how we can make deep architectures better.

With the basic idea of these two models out of the way, let’s jump into the code!
Getting Familiar with Our Dataset

For today’s task, we’ll be using a simple binary classification Dogs & Cats dataset from Kaggle. This 217.78 MB dataset contains 10,000 images of cats and dogs, split in an 80-20 training to test ratio. The training set contains 4000 cat and 4000 dog images, while the test set contains 1000 cat images and 1000 dog images each. There are two reasons for the use of a smaller dataset:

    Fine-tuning our classifier will take less time
    To showcase how fast a pretrained model adapts to a new dataset with less data

Configuring the Prerequisites

To begin with, let’s move into the config.py
script stored in the pyimagesearch
directory. This script will house the complete training and inference pipeline configuration values.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# import the necessary packages
import torch
import os
# define the parent data dir followed by the training and test paths
BASE_PATH = "dataset"
TRAIN_PATH = os.path.join(BASE_PATH, "training_set")
TEST_PATH = os.path.join(BASE_PATH, "test_set")
# specify ImageNet mean and standard deviation
MEAN = [0.485, 0.456, 0.406]
STD = [0.229, 0.224, 0.225]
# specify training hyperparameters
IMAGE_SIZE = 256
BATCH_SIZE = 128
PRED_BATCH_SIZE = 4
EPOCHS = 15
LR = 0.0001
# determine the device type 
DEVICE = torch.device("cuda") if torch.cuda.is_available() else "cpu"
# define paths to store training plot and trained model
PLOT_PATH = os.path.join("output", "model_training.png")
MODEL_PATH = os.path.join("output", "model.pth")

We start by initializing the base training of our dataset on Line 6. Then, on Lines 7 and 8, we use os.path.join
to specify our dataset’s training and test folders.

On Lines 11 and 12, we specify the ImageNet mean and standard deviations required later while creating our dataset instances. This is done because the models are pre-trained data preprocessed by these mean and standard deviation values, and we will try to make our present data as similar to the previously trained-on data as we can.

Next, we assign values to hyperparameters like image size, batch size, epochs, etc. (Lines 15-19) and determine the device we will train our models (Line 22).

We end our script by specifying the paths where our training plot and trained model weights will be stored (Lines 25 and 26).
Creating Utility Functions for Our Data Pipeline

We have created some functions to help us in the data pipeline and grouped them in the datautils.py
script to better work with our data.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# import the necessary packages
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from torch.datasets import Subset
def get_dataloader(dataset, batchSize, shuffle=True):
	# create a dataloader
	dl = DataLoader(dataset, batch_size=batchSize, shuffle=shuffle)
	# return the data loader
	return dl

Our first Utility function is the get_dataloader
function. It takes the dataset, batch size, and a Boolean variable shuffle
as its parameters (Line 6) and returns a PyTorch dataloader
instance (Line 11).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
def train_val_split(dataset, valSplit=0.2):
	# grab the total size of the dataset
	totalSize = len(dataset)
	# perform training and validation split
	(trainIdx, valIdx) = train_test_split(list(range(totalSize)),
		test_size=valSplit)
	trainDataset = Subset(dataset, trainIdx)
	valDataset = Subset(dataset, valIdx)
	# return training and validation dataset
	return (trainDataset, valDataset)

Next, we create a function called train_val_split
, which takes in the dataset and a validation split percentage variable as parameters (Line 13). Since our dataset has only training and test directories, we use the PyTorch dataset
subset feature to split the training set into a training and validation set.

We achieve this by first creating indexes for our split using the train_test_split
function and then assigning those indexes to the subsets (Lines 20 and 21). This function will return the train and validation data subsets (Line 24).
Creating the Classifier for Our Task

Our next task is to create a classifier for the cats and dogs dataset. Remember that we are not training our called model from scratch but fine-tuning it. For that, we’ll move on to the next script, which is classifier.py
.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# import the necessary packages
from torch.nn import Linear
from torch.nn import Module
class Classifier(Module):
	def __init__(self, baseModel, numClasses, model):
		super().__init__()
		# initialize the base model 
		self.baseModel = baseModel
		# check if the base model is VGG, if so, initialize the FC
		# layer accordingly
		if model == "vgg":
			self.fc = Linear(baseModel.classifier[6].out_features, 
                                numClasses)
		# otherwise, the base model is of type ResNet so initialize
		# the FC layer accordingly
		else:
			self.fc = Linear(baseModel.fc.out_features, numClasses)

In our Classifier
module (Line 5), the constructor function takes in the following arguments:

    baseModel
    : Since we’ll be calling either the VGG or the ResNet model, we will have already covered the bulk of our architecture. We will plug the called base model directly into our architecture on Line 9.
    numClasses
    : Will determine our architecture’s output nodes. For our task, the value is 2.
    model
    : A String variable that will tell us if our base model is VGG or a ResNet. Since we have to create a separate output layer specific to our task, we have to take the output of the final linear layer of the models. However, each model has a different method to access the final linear layer. Hence this model
    variable will help choose the model-specific method accordingly (Line 14 and Line 20). 

Notice how for VGGnet, we are using the command baseModel.classifier[6].out_features
whereas for ResNet, we are using baseModel.fc.out_features
. This is because these models have different named modules and layers. So we have to use different commands to access the last layers of each. Hence, the model
variable is very important for our code to work.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
	def forward(self, x):
		# pass the inputs through the base model to get the features
		# and then pass the features through of fully connected layer
		# to get our output logits
		features = self.baseModel(x)
		logits = self.fc(features)
		# return the classifier outputs
		return logits

Moving on to the forward
function, we simply get the output of the base model on Line 26 and pass it through our final fully connected layer (Line 27) to get the model output.
Training Our Custom Classifier

With the prerequisites out of the way, we move on to train.py
. First, we’ll train our classifier to differentiate between cats and dogs.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# USAGE
# python train.py --model vgg
# python train.py --model resnet
# import the necessary packages
from pyimagesearch import config
from pyimagesearch.classifier import Classifier
from pyimagesearch.datautils import get_dataloader 
from pyimagesearch.datautils import train_val_split
from torchvision.datasets import ImageFolder
from torchvision.transforms import Compose
from torchvision.transforms import ToTensor
from torchvision.transforms import RandomResizedCrop
from torchvision.transforms import RandomHorizontalFlip
from torchvision.transforms import RandomRotation
from torchvision.transforms import Normalize
from torch.nn import CrossEntropyLoss
from torch.nn import Softmax
from torch import optim
from tqdm import tqdm
import matplotlib.pyplot as plt
import argparse
import torch
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", type=str, default="vgg",
	choices=["vgg", "resnet"], help="name of the backbone model")
args = vars(ap.parse_args())

On Lines 6-23, we have all the required imports for our training module. Unsurprisingly, it’s quite a long list!

For ease of access and choice, we create an argument parser on Line 26, adding the argument model choice (either VGG or ResNet) on Lines 27-29.

The next series of code blocks are very important parts of our project. For example, for fine-tuning a model, we normally freeze the layers of our pre-trained model. However, while ablating different scenarios, we noticed that keeping the convolution layers frozen but the fully connected layers unfreeze for further training have helped our results.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# check if the name of the backbone model is VGG
if args["model"] == "vgg":
	# load VGG-11 model
	baseModel = torch.hub.load("pytorch/vision:v0.10.0", "vgg11",
		pretrained=True, skip_validation=True)
	# freeze the layers of the VGG-11 model
	for param in baseModel.features.parameters():
		param.requires_grad = False

The VGG model architecture called from Torch Hub (Lines 34 and 35) is split into several sub-modules for easier access. The Convolution layers are grouped under a module named features
, whereas the following fully connected layers are grouped under a module called classifier
. Since we just need to freeze the convolution layers, we directly access the parameters on Line 38 and freeze them by setting requires_grad
to False
, leaving the classifier
module layers untouched.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# otherwise, the backbone model we will be using is a ResNet
elif args["model"] == "resnet":
	# load ResNet 18 model
	baseModel = torch.hub.load("pytorch/vision:v0.10.0", "resnet18",
		pretrained=True, skip_validation=True)
	
	# define the last and the current layer of the model
	lastLayer = 8
	currentLayer = 1
	# loop over the child layers of the model
	for child in baseModel.children():
		# check if we haven't reached the last layer
		if currentLayer < lastLayer:
			# loop over the child layer's parameters and freeze them
			for param in child.parameters():
				param.requires_grad = False
		# otherwise, we have reached the last layers so break the loop
		else:
			break
		# increment the current layer
		currentLayer += 1   

In the case of the base model being ResNet, there are several ways we can approach this. The main thing to remember is that in ResNet, we only have to keep the single last fully connected layer unfrozen. Accordingly, on Lines 48 and 49, we set up the last layer and current layer index.

Looping over the available layers of ResNet on Line 52, we freeze all the layers, except for the last one (Lines 55-65).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# define the transform pipelines
trainTransform = Compose([
	RandomResizedCrop(config.IMAGE_SIZE),
	RandomHorizontalFlip(),
	RandomRotation(90),
	ToTensor(),
	Normalize(mean=config.MEAN, std=config.STD)
])
# create training dataset using ImageFolder
trainDataset = ImageFolder(config.TRAIN_PATH, trainTransform)

We proceed to create the input pipeline, starting with the PyTorch transform
instance, which automatically resizes, normalizes, and augments data without much hassle (Lines 68-74).

We top it off by using another great PyTorch utility function called ImageFolder
which will automatically create the input and target data, provided the directory is set up correctly (Line 77).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# create training and validation data split
(trainDataset, valDataset) = train_val_split(dataset=trainDataset)
# create training and validation data loaders
trainLoader = get_dataloader(trainDataset, config.BATCH_SIZE)
valLoader = get_dataloader(valDataset, config.BATCH_SIZE)

Using our train_val_split
utility function, we split the training dataset into a training and validation set (Line 80). Next, we use the get_dataloader
utility function from datautils.py
to create our data’s PyTorch dataloader
instances (Lines 83 and 84). This will allow us to feed data to our model in a generator-like way seamlessly.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# build the custom model
model = Classifier(baseModel=baseModel.to(config.DEVICE),
	numClasses=2, model=args["model"])
model = model.to(config.DEVICE)
# initialize loss function and optimizer
lossFunc = CrossEntropyLoss()
lossFunc.to(config.DEVICE)
optimizer = optim.Adam(model.parameters(), lr=config.LR)
# initialize the softmax activation layer
softmax = Softmax()

Moving on to our model prerequisites, we create our custom classifier and load it onto our device (Lines 87-89).

We have used cross-entropy as our loss function and Adam optimizer for today’s task (Lines 92-94). In addition, we use a separate softmax
loss to help with our training loss additions (Line 97).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# calculate steps per epoch for training and validation set
trainSteps = len(trainDataset) // config.BATCH_SIZE
valSteps = len(valDataset) // config.BATCH_SIZE
# initialize a dictionary to store training history
H = {
	"trainLoss": [],
	"trainAcc": [],
	"valLoss": [],
	"valAcc": []
}

The final step before the training epoch is to set up the training step and validation step values, followed by creating a dictionary that will store all the training history (Lines 100-109).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# loop over epochs
print("[INFO] training the network...")
for epoch in range(config.EPOCHS):
	# set the model in training mode
	model.train()
	
	# initialize the total training and validation loss
	totalTrainLoss = 0
	totalValLoss = 0
	
	# initialize the number of correct predictions in the training
	# and validation step
	trainCorrect = 0
	valCorrect = 0

Inside the training loop, we first set our model to training mode (Line 115). Next, we initialize the training loss, validation loss, training, and validation accuracy variables (Lines 118-124).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
	# loop over the training set
	for (image, target) in tqdm(trainLoader):
		# send the input to the device
		(image, target) = (image.to(config.DEVICE),
			target.to(config.DEVICE))
		
		# perform a forward pass and calculate the training loss
		logits = model(image)
		loss = lossFunc(logits, target)
		# zero out the gradients, perform the backpropagation step,
		# and update the weights
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
		# add the loss to the total training loss so far, pass the
		# output logits through the softmax layer to get output
		# predictions, and calculate the number of correct predictions
		totalTrainLoss += loss.item()
		pred = softmax(logits)
		trainCorrect += (pred.argmax(dim=-1) == target).sum().item()

Looping over the complete training set, we first load the data and target to the device (Lines 129 and 130). Next, we simply pass the data through the model and get the output, followed by plugging the predictions and target into our loss function (Lines 133 and 134),

Lines 138-140 is the standard PyTorch backpropagation step, where we zero out the gradients, perform backpropagation, and update the weights.

Next, we add the loss to our total training loss (Line 145), pass the model output through a softmax to get the isolated prediction value, which we then add to the trainCorrect
variable.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
	# switch off autograd
	with torch.no_grad():
		# set the model in evaluation mode
		model.eval()
		
		# loop over the validation set
		for (image, target) in tqdm(valLoader):
			# send the input to the device
			(image, target) = (image.to(config.DEVICE),
				target.to(config.DEVICE))
			# make the predictions and calculate the validation
			# loss
			logits = model(image)
			valLoss = lossFunc(logits, target)
			totalValLoss += valLoss.item()
			
			# pass the output logits through the softmax layer to get
			# output predictions, and calculate the number of correct
			# predictions
			pred = softmax(logits)
			valCorrect += (pred.argmax(dim=-1) == target).sum().item()

Most of the steps involved in the validation process are the same as the training process, except for a few things:

    The model is set to evaluation mode (Line 152)
    There is no updating of weights

→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
	# calculate the average training and validation loss
	avgTrainLoss = totalTrainLoss / trainSteps
	avgValLoss = totalValLoss / valSteps
	
	# calculate the training and validation accuracy
	trainCorrect = trainCorrect / len(trainDataset)
	valCorrect = valCorrect / len(valDataset)
	# update our training history
	H["trainLoss"].append(avgTrainLoss)
	H["valLoss"].append(avgValLoss)
	H["trainAcc"].append(trainCorrect)
	H["valAcc"].append(valCorrect)
	# print the model training and validation information
	print(f"[INFO] EPOCH: {epoch + 1}/{config.EPOCHS}")
	print(f"Train loss: {avgTrainLoss:.6f}, Train accuracy: {trainCorrect:.4f}")
	print(f"Val loss: {avgValLoss:.6f}, Val accuracy: {valCorrect:.4f}")

Before exiting the training loop, we calculate the average losses (Lines 173 and 174) and the training and validation accuracy (Lines 177 and 178).

We then proceed to append those values to our training history dictionary (Lines 181-184).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()
plt.plot(H["trainLoss"], label="train_loss")
plt.plot(H["valLoss"], label="val_loss")
plt.plot(H["trainAcc"], label="train_acc")
plt.plot(H["valAcc"], label="val_acc")
plt.title("Training Loss and Accuracy on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")
plt.savefig(config.PLOT_PATH)
# serialize the model state to disk
torch.save(model.module.state_dict(), config.MODEL_PATH)

Before finishing our training script, we plot all the training dictionary variables (Lines 192-201) and save the figure (Line 202).

Our final task is to save the model weights in the previously defined path (Line 205).

Let’s see how the values per epoch look!
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
[INFO] training the network...
100%|██████████| 50/50 [01:24<00:00,  1.68s/it]
100%|██████████| 13/13 [00:19<00:00,  1.48s/it]
[INFO] EPOCH: 1/15
Train loss: 0.289117, Train accuracy: 0.8669
Val loss: 0.217062, Val accuracy: 0.9119
100%|██████████| 50/50 [00:47<00:00,  1.05it/s]
100%|██████████| 13/13 [00:11<00:00,  1.10it/s]
[INFO] EPOCH: 2/15
Train loss: 0.212023, Train accuracy: 0.9039
Val loss: 0.223640, Val accuracy: 0.9025
100%|██████████| 50/50 [00:46<00:00,  1.07it/s]
100%|██████████| 13/13 [00:11<00:00,  1.15it/s]
[INFO] EPOCH: 3/15
...
Train loss: 0.139766, Train accuracy: 0.9358
Val loss: 0.187595, Val accuracy: 0.9194
100%|██████████| 50/50 [00:46<00:00,  1.07it/s]
100%|██████████| 13/13 [00:11<00:00,  1.15it/s]
[INFO] EPOCH: 13/15
Train loss: 0.134248, Train accuracy: 0.9425
Val loss: 0.146280, Val accuracy: 0.9437
100%|██████████| 50/50 [00:47<00:00,  1.05it/s]
100%|██████████| 13/13 [00:11<00:00,  1.12it/s]
[INFO] EPOCH: 14/15
Train loss: 0.132265, Train accuracy: 0.9428
Val loss: 0.162259, Val accuracy: 0.9319
100%|██████████| 50/50 [00:47<00:00,  1.05it/s]
100%|██████████| 13/13 [00:11<00:00,  1.16it/s]
[INFO] EPOCH: 15/15
Train loss: 0.138014, Train accuracy: 0.9409
Val loss: 0.153363, Val accuracy: 0.9313

Our pretrained model accuracy is already near 90% in the very first epoch. By the 13th epoch, the values had saturated at about ~94%. To put this in perspective, a pretrained model trained on a different dataset starts at about 86% accuracy on a dataset it hasn’t seen before. That’s how well it has learned to extract features.

A complete overview of the metrics has been plotted in Figure 5.
Figure 5: Training and Validation metric plots.
Putting Our Fine-Tuned Model to the Test

With our model ready for use, we’ll move on to our inference script, inference.py
.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# USAGE
# python inference.py --model vgg
# python inference.py --model resnet
# import the necessary packages
from pyimagesearch import config
from pyimagesearch.classifier import Classifier
from pyimagesearch.datautils import get_dataloader
from torchvision.datasets import ImageFolder
from torchvision.transforms import Compose
from torchvision.transforms import ToTensor
from torchvision.transforms import Resize
from torchvision.transforms import Normalize
from torchvision import transforms
from torch.nn import Softmax
from torch import nn
import matplotlib.pyplot as plt
import argparse
import torch
import tqdm
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", type=str, default="vgg",
	choices=["vgg", "resnet"], help="name of the backbone model")
args = vars(ap.parse_args())

Since we’ll have to initialize our model before loading in the weights, we’ll be needing the right model argument. For that, we have created an argument parser on Lines 23-26.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# initialize test transform pipeline
testTransform = Compose([
	Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
	ToTensor(),
	Normalize(mean=config.MEAN, std=config.STD)
])
# calculate the inverse mean and standard deviation
invMean = [-m/s for (m, s) in zip(config.MEAN, config.STD)]
invStd = [1/s for s in config.STD]
# define our denormalization transform
deNormalize = transforms.Normalize(mean=invMean, std=invStd)
# create the test dataset
testDataset = ImageFolder(config.TEST_PATH, testTransform)
# initialize the test data loader
testLoader = get_dataloader(testDataset, config.PRED_BATCH_SIZE)

Since we’ll be calculating the accuracy of our model over the complete test data set, we create a PyTorch transform
instance for our test data on Lines 29-33.

Consequently, we calculate the inverse mean and inverse standard deviation values, which we use to create a transforms.Normalize
instance (Lines 36-40). This is done because the data is preprocessed before getting fed to the model. For display purposes, we have to revert the images to their original state.

Using the ImageFolder
utility function, we create our test dataset instance and feed it to the get_dataloader
function we had created previously for a test dataLoader
instance (Lines 43-46).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# check if the name of the backbone model is VGG
if args["model"] == "vgg":
	# load VGG-11 model
	baseModel = torch.hub.load("pytorch/vision:v0.10.0", "vgg11",
		pretrained=True, skip_validation=True)
# otherwise, the backbone model we will be using is a ResNet
elif args["model"] == "resnet":
	# load ResNet 18 model
	baseModel = torch.hub.load("pytorch/vision:v0.10.0", "resnet18",
		pretrained=True, skip_validation=True)

As mentioned before, since we have to initialize the model again, we check for the model argument given and accordingly use Torch Hub to load the model (Lines 49-58).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# build the custom model
model = Classifier(baseModel=baseModel.to(config.DEVICE),
	numClasses=2, vgg = False)
model = model.to(config.DEVICE)
# load the model state and initialize the loss function
model.load_state_dict(torch.load(config.MODEL_PATH))
lossFunc = nn.CrossEntropyLoss()
lossFunc.to(config.DEVICE)
# initialize test data loss
testCorrect = 0
totalTestLoss = 0
soft = Softmax()

On Lines 61-66, we initialize the model, store it on our device, and load the previously obtained weights during model training.

As we had done in the train.py
script, we choose Cross-Entropy as our loss function (Line 67) and initialize the test loss and accuracy (Lines 71 and 72).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# switch off autograd
with torch.no_grad():
	# set the model in evaluation mode
	model.eval()
	# loop over the validation set
	for (image, target) in tqdm(testLoader):
		# send the input to the device
		(image, target) = (image.to(config.DEVICE), 
			target.to(config.DEVICE))
		# make the predictions and calculate the validation
		# loss
		logit = model(image)
		loss = lossFunc(logit, target)
		totalTestLoss += loss.item()
		# output logits through the softmax layer to get output
		# predictions, and calculate the number of correct predictions
		pred = soft(logit)
		testCorrect += (pred.argmax(dim=-1) == target).sum().item()

With automatic gradients off (Line 76), we set the model to evaluation mode on Line 78. Then, looping over the test images, we feed them to the model and pass the prediction and targets through the loss function (Lines 81-89).

The accuracy is calculated by passing the predictions through a softmax function (Lines 95 and 96).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# print test data accuracy		
print(testCorrect/len(testDataset))
# initialize iterable variable
sweeper = iter(testLoader)
# grab a batch of test data
batch = next(sweeper)
(images, labels) = (batch[0], batch[1])
# initialize a figure
fig = plt.figure("Results", figsize=(10, 10 ))

Now we’ll look at some specific cases of test data and display them. For that reason, we initialize an iterable variable on Line 102 and grab a batch of data (Line 105).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #2: VGG and ResNet
# switch off autograd
with torch.no_grad():
	# send the images to the device
	images = images.to(config.DEVICE)
	# make the predictions
	preds = model(images)
	# loop over all the batch
	for i in range(0, config.PRED_BATCH_SIZE):
		# initialize a subplot
		ax = plt.subplot(config.PRED_BATCH_SIZE, 1, i + 1)
		# grab the image, de-normalize it, scale the raw pixel
		# intensities to the range [0, 255], and change the channel
		# ordering from channels first tp channels last
		image = images[i]
		image = deNormalize(image).cpu().numpy()
		image = (image * 255).astype("uint8")
		image = image.transpose((1, 2, 0))
		# grab the ground truth label
		idx = labels[i].cpu().numpy()
		gtLabel = testDataset.classes[idx]
		# grab the predicted label
		pred = preds[i].argmax().cpu().numpy()
		predLabel = testDataset.classes[pred]
		# add the results and image to the plot
		info = "Ground Truth: {}, Predicted: {}".format(gtLabel,
				predLabel)
		plt.imshow(image)
		plt.title(info)
		plt.axis("off")
	
	# show the plot
	plt.tight_layout()
	plt.show()

We turn automatic gradients off again and make predictions of the batch of data previously grabbed (Lines 112-117).

Looping over the batch, we grab individual images, denormalize them, rescale them, and fix their dimensions to make them displayable (Lines 127-130).

Based on the image currently under consideration, we first grab its ground truth labels (Lines 133 and 134) and their corresponding predicted labels on Lines 137 and 138 and display them accordingly (Lines 141-145).
Results from Our Fine-Tuned Model

On the overall test dataset, our ResNet-backed custom model yielded an accuracy of 97.5%. In Figures 6-9, we see a batch of data displayed, along with their corresponding ground truth and predicted labels.
Figure 6: An image of a cat, predicted as a cat.
Figure 7: An image of a dog, predicted as a dog.
Figure 8: An image of a cat, predicted as a cat.
Figure 9: An image of a cat, predicted as a cat.

With 97.5% accuracy, you can be assured that this performance level is not just for this batch but for all batches. You can repeatedly run the sweeper
variable to get a different data batch to see for yourself.
What's next? I recommend PyImageSearch University.
Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled

I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

    ✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
    ✓ 30+ Certificates of Completion
    ✓ 39h 44m on-demand video
    ✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
    ✓ Pre-configured Jupyter Notebooks in Google Colab
    ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
    ✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
    ✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
    ✓ Access on mobile, laptop, desktop, etc.

Click here to join PyImageSearch University
Summary

Today’s tutorial not only served as a showcase for how you can utilize Torch Hub’s gallery of models, but it also stands as another reminder of how much pre-trained models help us in our daily machine learning escapades.

Imagine if you had to train a big architecture like ResNet from scratch for any task of your choice. It would take much more time and definitely many more epochs. By this point, you surely have to appreciate the idea behind PyTorch Hub to make the whole process of using these state-of-the-art models much more efficient.

Continuing from where we left in our previous week’s tutorial, I would like to emphasize that PyTorch Hub is still rough around the edges, and it still has a huge scope of improvement. Still, surely we are getting closer to the perfect version!
Citation Information

Chakraborty, D. “Torch Hub Series #2: VGG and ResNet,” PyImageSearch, 2021, https://www.pyimagesearch.com/2021/12/27/torch-hub-series-2-vgg-and-resnet/
@article{dev_2021_THS2,
  author = {Devjyoti Chakraborty},
  title = {{Torch Hub} Series \#2: {VGG} and {ResNet}},
  journal = {PyImageSearch},
  year = {2021},
  note = {https://www.pyimagesearch.com/2021/12/27/torch-hub-series-2-vgg-and-resnet/},
}
Want free GPU credits to train models?

    We used Jarvislabs.ai, a GPU cloud, for all the experiments.
    We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit here.

In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.

To overcome this problem, we use GPUs. The problem is these GPUs are expensive and become outdated quickly.

GPUs are great because they take your Neural Network and train it quickly. The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally. Cloud GPUs let you use a GPU and only pay for the time you are running the GPU. It’s a brilliant idea that saves you money.

JarvisLabs provides the best-in-class GPUs, and PyImageSearch University students get between 10 - 50 hours on a world-class GPU (time depends on the specific GPU you select).

This gives you a chance to test-drive a monstrously powerful GPU on any of our tutorials in a jiffy. So join PyImageSearch University today and try for yourself.

Click here to get Jarvislabs credits now

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!
Download the Source Code and FREE 17-page Resource Guide

Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!
About the Author

Hey, I'm Devjyoti and I joined the ML bandwagon because it was too good to resist. Throughout my ML journey, I have been stuck many times while understanding concepts. I want to present those concepts to you in a way I wish they were presented to me, so the learning process becomes easier!
Reader Interactions

Previous Article:
Torch Hub Series #1: Introduction to Torch Hub

Next Article:
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Comment section

Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.
Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
Join Now
Picked For You
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Torch Hub Series #4: PGAN — Model on GAN
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Torch Hub Series #1: Introduction to Torch Hub
U-Net: Training Image Segmentation Models in PyTorch
Similar articles
Deep Learning
Tutorials
Implementing the Perceptron Neural Network with Python
May 6, 2021

Deep Learning
Keras and TensorFlow
Object Detection
Tutorials
Multi-class object detection and bounding box regression with Keras, TensorFlow, and Deep Learning
October 12, 2020

Tutorials
Skin Detection: A Step-by-Step Example using Python and OpenCV
August 18, 2014

You can learn Computer Vision, Deep Learning, and OpenCV.

Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you’ll find our hand-picked tutorials, books, courses, and libraries to help you master CV and DL.
Footer
Topics

    Deep Learning
    Dlib Library
    Embedded/IoT and Computer Vision
    Face Applications
    Image Processing
    Interviews
    Keras

    Machine Learning and Computer Vision
    Medical Computer Vision
    Optical Character Recognition (OCR)
    Object Detection
    Object Tracking
    OpenCV Tutorials
    Raspberry Pi

Books & Courses

    FREE CV, DL, and OpenCV Crash Course
    Practical Python and OpenCV
    Deep Learning for Computer Vision with Python
    PyImageSearch Gurus Course
    Raspberry Pi for Computer Vision

PyImageSearch

    Get Started
    OpenCV Install Guides
    About
    FAQ
    Blog
    Contact
    Privacy Policy

© 2022 PyImageSearch. All Rights Reserved.



########## FOOBAR___midas_inference >>__PyImageSearch>> Torch Hub Series #5: MiDaS — Model on Depth Estimation

SOURCE >> https://www.pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/


Deep Learning Depth Estimation MiDaS PyTorch Tutorials
Torch Hub Series #5: MiDaS — Model on Depth Estimation

by Devjyoti Chakraborty on January 17, 2022
Click here to download the source code to this post

In the previous part of this series, we discussed some state-of-the-art object detection models; YOLOv5 and SSD. In today’s tutorial, we will discuss MiDaS, an ingenious attempt to aid the depth estimation of images.

With this tutorial, we will create a basic intuition about the idea behind MiDaS and learn how to use it as a depth estimation inference tool.

This lesson is part 5 of a 6-part series on Torch Hub:

    Torch Hub Series #1: Introduction to Torch Hub
    Torch Hub Series #2: VGG and ResNet
    Torch Hub Series #3: YOLO v5 and SSD — Models on Object Detection
    Torch Hub Series #4: PGAN — Model on GAN
    Torch Hub Series #5: MiDaS — Model on Depth Estimation (this tutorial)
    Torch Hub Series #6: Image Segmentation

To learn how to use MiDaS on your custom data, just keep reading.
Looking for the source code to this post?
Jump Right To The Downloads Section
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Introduction

First, let us understand what depth estimation is or why it is important. Depth estimation of an image predicts the order of objects (if the image was expanded in a 3D format) from the 2D image itself. It is an unequivocally difficult task since getting annotated data and datasets specializing in this area was a mammoth task itself. The use of depth estimation is far and wide, most noticeably in the domain of self-driving cars, where estimating the distance of objects around a car helps in navigation (Figure 1).
Figure 1: Depth maps (source).

The researchers behind MiDaS have explained their motives in a very simple manner. They firmly assert that training models on a single dataset will not be robust when dealing with problem statements encompassing real-life issues. When models used in real-time are created, they should be robust enough to deal with as many situations and outliers as possible.

Keeping that in mind, the creators of MiDaS decided to train their model on multiple datasets. This includes datasets with different types of labels and objective functions. To achieve this, they devised a method to carry out computations in an appropriate output space compatible with all ground-truth representations.

The idea is very ingenious on paper, but the authors had to carefully devise loss functions and consider the challenges accompanying the choice of using multiple datasets. Since these datasets had different representations of depth estimation to varying degrees, an inherent scale ambiguity, and shift ambiguity come up, as the paper’s authors addressed.

Now, since the datasets in all probability would follow distributions different from each other. Hence the issues are pretty much expected. However, the authors propose solutions to each of the challenges. The end product was a robust depth estimator, which was as efficient as accurate. In Figure 2, we see some results shown in the paper.
Figure 2: Some results from Ranftl et al. (2020).

The idea of cross dataset learning isn’t new, but the complications that arise from getting the ground truths to a common output space are extremely tough to get by. However, the paper explains each step extensively, starting with the intuition right to the mathematical definition of the losses used.

Let’s see how we can use the MiDaS model to find the inverse depth of our custom images.
Configuring Your Development Environment

To follow this guide, you need to have the OpenCV library installed on your system.

Luckily, OpenCV is pip-installable:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
$ pip install opencv-contrib-python

If you need help configuring your development environment for OpenCV, I highly recommend that you read my pip install OpenCV guide — it will have you up and running in a matter of minutes.
Having Problems Configuring Your Development Environment?
Figure 3: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.

All that said, are you:

    Short on time?
    Learning on your employer’s administratively locked system?
    Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
    Ready to run the code right now on your Windows, macOS, or Linux system?

Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!
Project Structure

We first need to review our project directory structure.

Start by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.

From there, take a look at the directory structure:
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
!tree .
.
├── midas_inference.py
├── output
│   └── midas_output
│       └── output.png
└── pyimagesearch
    ├── config.py
    └── data_utils.py

Inside the pyimagesearch
directory, we have 2 scripts:

    config.py
    : Contains an end to end configuration pipeline for the project
    data_utils.py
    : Houses the two data utility functions we’ll be using in the project

In the parent directory, we have one single script:

    midas_inference.py
    : To infer from the pretrained MiDaS model

Finally, we have the output
directory, which will house the result plots obtained from running the script.
Downloading the Dataset

Owing to its compactness, we’ll be using the Dogs & Cats Images dataset from Kaggle again.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
$ mkdir ~/.kaggle
$ cp <path to your kaggle.json> ~/.kaggle/
$ chmod 600 ~/.kaggle/kaggle.json
$ kaggle datasets download -d chetankv/dogs-cats-images
$ unzip -qq dogs-cats-images.zip
$ rm -rf "/content/dog vs cat"

As explained in previous posts of this series, you’ll need your own unique kaggle.json
file to connect to the Kaggle API (Line 2). The chmod 600
command on Line 3 will allow your script complete access to read and write files.

The following kaggle datasets download
command (Line 4) allows you to download any dataset hosted on their website. Finally, we have the unzip command and an auxiliary delete command for the unnecessary additions (Lines 5 and 6).

Let’s move on to the configuration pipeline.
Configuring the Prerequisites

Inside the pyimagesearch
directory, you’ll find a script called config.py
. This script will house the complete end-to-end configuration pipeline of our project.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# import the necessary packages
import torch
import os
# define the root directory followed by the test dataset paths
BASE_PATH = "dataset"
TEST_PATH = os.path.join(BASE_PATH, "test_set")
# specify image size and batch size
IMAGE_SIZE = 384
PRED_BATCH_SIZE = 4
# determine the device type 
DEVICE = torch.device("cuda") if torch.cuda.is_available() else "cpu"
# define paths to save output 
OUTPUT_PATH = "output"
MIDAS_OUTPUT = os.path.join(OUTPUT_PATH, "midas_output")

First, we have the BASE_PATH
variable as a pointer to the dataset directory (Line 6). We’re not doing any additional tinkering to our models, so we’ll only use the test set (Line 7).

On Line 10, we have a variable named IMAGE_SIZE
, set to 384
, as a mandate for our MiDaS model input. The prediction batch size is set to 4
(Line 11), but readers are encouraged to play around with different sizes.

It’s advisable that you have a CUDA-compatible device for today’s project (Line 14), but since we’re not going for any heavy training, CPUs should work fine too.

Lastly, we have created paths to save the outputs obtained from the model inferences (Lines 17 and 18).

We’ll only be using a single helper function to aid our pipeline in today’s task. For that, we’ll move onto the second script in the pyimagesearch
directory, data_utils.py
.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# import the necessary packages
from torch.utils.data import DataLoader
def get_dataloader(dataset, batchSize, shuffle=True):
	# create a dataloader and return it
	dataLoader= DataLoader(dataset, batch_size=batchSize,
		shuffle=shuffle)
	return dataLoader

On Line 4, we have the get_dataloader
function, which takes in the dataset, batch size, and shuffle variables as its arguments. This function returns a generator like the PyTorch Dataloader instance which will help us deal with huge data (Line 6).

That concludes our utilities. Let’s move on to the inference script.
Finding Inverse Depth Estimation Using MiDaS

At this time, a very logical question might pop up in your mind; Why are we going to such lengths to get inference on a handful of images?

The path we have chosen here is a full-proof way of dealing with huge datasets, and the pipeline will be useful even if you choose to train the model for fine-tuning later down the road. We have also considered preparing the data as best we can without invoking the premade functions of the MiDaS repository.
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# import necessary packages
from pyimagesearch.data_utils import get_dataloader
from pyimagesearch import config
from torchvision.transforms import Compose, ToTensor, Resize
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
import torch
import os
# create the test dataset with a test transform pipeline and
# initialize the test data loader
testTransform = Compose([
	Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)), ToTensor()])
testDataset = ImageFolder(config.TEST_PATH, testTransform)
testLoader = get_dataloader(testDataset, config.PRED_BATCH_SIZE)

As mentioned earlier, since we will be only using the test set, we have created a PyTorch test transform instance where we are reshaping the images and converting them to tensors (Lines 12 and 13).

If your dataset is in the format as the one we are using for our project (i.e., images under the folder named as labels), then we can use the ImageFolder
function to create a PyTorch Dataset instance (Line 14). Lastly, we use the previously defined get_dataloader
function to generate a Dataloader instance (Line 15).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# initialize the midas model using torch hub
modelType = "DPT_Large" 
midas = torch.hub.load("intel-isl/MiDaS", modelType)
# flash the model to the device and set it to eval mode
midas.to(device)
midas.eval()

Next, we use the torch.hub.load
function to load the MiDaS model in our local runtime (Lines 18 and 19). Again, several available choices can be called here, all of which can be found here. Finally, we load the model to our device and set it to evaluation mode (Lines 22 and 23).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# initialize iterable variable
sweeper = iter(testLoader)
# grab a batch of test data send the images to the device
print("[INFO] getting the test data...")
batch = next(sweeper)
(images, _) = (batch[0], batch[1])
images = images.to(config.DEVICE) 
 
# turn off auto grad
with torch.no_grad():
	# get predictions from input
	prediction = midas(images)
	# unsqueeze the predictions batchwise
	prediction = torch.nn.functional.interpolate(
		prediction.unsqueeze(1), size=[384,384], mode="bicubic",
		align_corners=False).squeeze()
# store the predictions in a numpy array
output = prediction.cpu().numpy()

The sweeper
variable on Line 26 will act as the iterable variable of the testLoader
. Each time we run the command on Line 30, we’ll get a new batch of data from the testLoader
. On Line 31, we unpack the batch into images and labels, keeping only the images.

After loading the images to our device (Line 32), we turn off automatic gradients and pass the images through the model (Lines 35-37). On Line 40, we use a beautiful utility function called torch.nn.functional.interpolate
to unpack our predictions into a valid 3-channel image format.

Finally, we store the reformatted predictions to an output variable in numpy format (Line 45).
→ Launch Jupyter Notebook on Google Colab
Torch Hub Series #5: MiDaS — Model on Depth Estimation
# define row and column variables
rows = config.PRED_BATCH_SIZE
cols = 2
# define axes for subplots
axes = []
fig=plt.figure(figsize=(10, 20))
# loop over the rows and columns
for totalRange in range(rows*cols):
	axes.append(fig.add_subplot(rows, cols, totalRange+1))
	# set up conditions for side by side plotting 
	# of ground truth and predictions
	if totalRange % 2 == 0:
		plt.imshow(images[totalRange//2]
			.permute((1, 2, 0)).cpu().detach().numpy())
	else :
		plt.imshow(output[totalRange//2])
fig.tight_layout()
# build the midas output directory if not already present
if not os.path.exists(config.MIDAS_OUTPUT):
	os.makedirs(config.MIDAS_OUTPUT)
# save plots to output directory
print("[INFO] saving the inference...")
outputFileName = os.path.join(config.MIDAS_OUTPUT, "output.png")
plt.savefig(outputFileName)

To plot our results, we first define the rows and column variables to define the grid format (Lines 48 and 49). On Lines 52 and 53, we define the subplot list and figure size.

Looping over the rows and columns, we define a methodology where the inverse depth estimation and the ground truth images get plotted side by side (Lines 56-66).

Lastly, we save the figure to our desired path (Lines 69 and 75).

With our inference script done, let’s check out some of the results.
MiDaS Inference Results

Most of the images in our dataset will have either a cat or dog in the forefront. There might not be enough images with background, but the MiDaS model should give us an output that depicts the cat or dog in the foreground decisively. That is exactly what has happened in our inference images ( Figures 4-7).
Figure 4: Inverse depth estimation of an image of a cat closeup.
Figure 5: Inverse depth estimation of an image of a dog in a field.
Figure 6: Inverse depth estimation of an image of an indoor cat.
Figure 7: Inverse depth estimation of an image of a cat in a box.

Although the spectacular capabilities of MiDaS can be seen in all the inference images, we can look into each of them in-depth and draw out some more observations.

In Figure 4, not only is the cat depicted in the foreground, but its head is closer to the camera than its body (shown by the change in color). In Figure 5, since the field mainly covers the image, there is a clear distinction between the dog and the field. In both Figure 6 and Figure 7, the cat’s head is depicted closer than its body.
What's next? I recommend PyImageSearch University.
Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled

I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

    ✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
    ✓ 30+ Certificates of Completion
    ✓ 39h 44m on-demand video
    ✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
    ✓ Pre-configured Jupyter Notebooks in Google Colab
    ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
    ✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
    ✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
    ✓ Access on mobile, laptop, desktop, etc.

Click here to join PyImageSearch University
Summary

Grasping the importance of MiDaS in today’s world is an important step for us. Imagine how much help a perfect depth estimator would be for autonomous vehicles. Since autonomous cars almost completely depend on utilities like LiDAR (light detection and ranging), cameras, SONAR (sound navigation and ranging), etc., having a full-proof system of depth estimation of its surroundings will make travel safer and take the load off the other sensor systems.

The second most important thing to note about MiDaS is the usage of cross dataset mixing. While it has recently gained momentum, executing it to perfection takes considerable time and planning. Moreover, MiDaS does this on a domain that significantly impacts a real-world issue.

I hope this tutorial served as a gateway to pique your interest regarding all things about autonomous systems and depth estimation. Feel free to try this out with your custom datasets and share the results.
Citation Information

Chakraborty, D. “Torch Hub Series #5: MiDaS — Model on Depth Estimation,” PyImageSearch, 2022, https://www.pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/
@article{Chakraborty_2022_THS5,
  author = {Devjyoti Chakraborty},
  title = {Torch Hub Series \#5: {MiDaS} — Model on Depth Estimation},
  journal = {PyImageSearch},
  year = {2022},
  note = {https://www.pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/},
}
Want free GPU credits to train models?

    We used Jarvislabs.ai, a GPU cloud, for all the experiments.
    We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit here.

In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.

To overcome this problem, we use GPUs. The problem is these GPUs are expensive and become outdated quickly.

GPUs are great because they take your Neural Network and train it quickly. The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally. Cloud GPUs let you use a GPU and only pay for the time you are running the GPU. It’s a brilliant idea that saves you money.

JarvisLabs provides the best-in-class GPUs, and PyImageSearch University students get between 10 - 50 hours on a world-class GPU (time depends on the specific GPU you select).

This gives you a chance to test-drive a monstrously powerful GPU on any of our tutorials in a jiffy. So join PyImageSearch University today and try for yourself.

Click here to get Jarvislabs credits now

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!
Download the Source Code and FREE 17-page Resource Guide

Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!
About the Author

Hey, I'm Devjyoti and I joined the ML bandwagon because it was too good to resist. Throughout my ML journey, I have been stuck many times while understanding concepts. I want to present those concepts to you in a way I wish they were presented to me, so the learning process becomes easier!
Reader Interactions

Previous Article:
Torch Hub Series #4: PGAN — Model on GAN

Next Article:
Torch Hub Series #5: MiDaS — Model on Depth Estimation
Comment section

Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.
Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
Join Now
Picked For You
Torch Hub Series #4: PGAN — Model on GAN
Torch Hub Series #3: YOLOv5 and SSD — Models on Object Detection
Torch Hub Series #2: VGG and ResNet
Torch Hub Series #1: Introduction to Torch Hub
U-Net: Training Image Segmentation Models in PyTorch
Similar articles
Deep Learning
Tutorials
LeNet – Convolutional Neural Network in Python
August 1, 2016

Deep Learning
PyTorch
Tutorials
Torch Hub Series #1: Introduction to Torch Hub
December 20, 2021

Interviews
PyImageSearch Gurus
PyImageSearch Gurus member spotlight: Saideep Talari
June 12, 2017

You can learn Computer Vision, Deep Learning, and OpenCV.

Get your FREE 17 page Computer Vision, OpenCV, and Deep Learning Resource Guide PDF. Inside you’ll find our hand-picked tutorials, books, courses, and libraries to help you master CV and DL.
Footer
Topics

    Deep Learning
    Dlib Library
    Embedded/IoT and Computer Vision
    Face Applications
    Image Processing
    Interviews
    Keras

    Machine Learning and Computer Vision
    Medical Computer Vision
    Optical Character Recognition (OCR)
    Object Detection
    Object Tracking
    OpenCV Tutorials
    Raspberry Pi

Books & Courses

    FREE CV, DL, and OpenCV Crash Course
    Practical Python and OpenCV
    Deep Learning for Computer Vision with Python
    PyImageSearch Gurus Course
    Raspberry Pi for Computer Vision

PyImageSearch

    Get Started
    OpenCV Install Guides
    About
    FAQ
    Blog
    Contact
    Privacy Policy

© 2022 PyImageSearch. All Rights Reserved.



############

Hi,

I was really into skateboarding as a middle schooler. Every waking second I spent on my board.

One day, I gave myself a pretty bad concussion. I was coasting along, and wham, out of nowhere, a big rock lodges underneath one of my wheels, my board abruptly halts, and I go flying.

Luckily, my head broke the fall.
Image
I was out cold for a minute, but when I came to, I was disoriented. I didn’t know up from down or left from right. My brain needed a few minutes to reorient myself and figure out what was going on.

When applying OCR, knowing your orientation is critical. If your image is rotated, then your OCR accuracy will suffer.

Therefore, one way to improve OCR accuracy is to detect the text orientation and then correct for it:
Image
The big picture: If the text in your input image is rotated, your OCR accuracy will suffer. To improve OCR accuracy, you should first correct text orientation.

How it works: Tesseract’s orientation and script detection (OSD) mode lets you do text orientation correction out of the box.

Our thoughts: If you don’t have a way to deal with text orientation, you’ve got no chance of getting good OCR results. 

Yes, but: There are ways to correct text orientation, including various applications of deep learning; however, few are easier and more integrated than Tesseract’s OSD.

Stay smart: OSD won’t fix all your problems, but it will fix most of them when it comes to text orientation and OCR.

Click here to learn how to use OSD to correct text orientation with Tesseract.

PyImageSearch University
This lesson is part of PyImageSearch University, our flagship program to help you master computer vision, deep learning, and OpenCV.  PyImageSearch University is updated each week with new lessons.

Don’t know Python?  No problem, we’ve got you covered with a short and sweet Python course to get you going.

Having problems with your local development environment or IDE?  Fortunately, our pre-configured Colab Notebooks mean you can run code the moment you join PyImageSearch University.  But, of course, you don’t want to be a sys-admin, so don’t waste time messing with your development environment.  

You can find the current lesson under OCR 110 — Using Tesseract for Translation and Non-English Languages. The direct link can be found here.

Want to master computer vision and deep learning?
Do you think mastering computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case. All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what we do. Our mission is to change education and how complex Artificial Intelligence topics are taught.


Skip to primary navigation
Skip to main content
Skip to primary sidebar
Skip to footer
PYIMAGESEARCH

You can master Computer Vision, Deep Learning, and OpenCV - PyImageSearch
OpenCV Install Guides
About
FAQ
Contact
Get Started
Topics
Deep Learning
Dlib Library
Embedded/IoT and Computer Vision
Face Applications
Image Processing
Interviews
Keras and TensorFlow
Machine Learning and Computer Vision
Medical Computer Vision
Optical Character Recognition (OCR)
Object Detection
Object Tracking
OpenCV Tutorials
Raspberry Pi
Books and Courses
Student Success Stories
Blog
Search
OPTICAL CHARACTER RECOGNITION (OCR) TUTORIALS

Correcting Text Orientation with Tesseract and Python
by Adrian Rosebrock on January 31, 2022

Click here to download the source code to this post

An essential component of any OCR system is image preprocessing — the higher the quality input image you present to the OCR engine, the better your OCR output will be. To be successful in OCR, you need to review arguably the most important pre-processing step: text orientation.


To learn how to perform text orientation with Tesseract and Python, just keep reading.


Looking for the source code to this post?
JUMP RIGHT TO THE DOWNLOADS SECTION 
Correcting Text Orientation with Tesseract and Python
Text orientation refers to the rotation angle of a piece of text in an image. A given word, sentence, or paragraph will look like gibberish to an OCR engine if the text is significantly rotated. OCR engines are intelligent, but like humans, they are not trained to read upside-down!

Therefore, a critical first step in preparing your image data for OCR is to detect text orientation (if any) and then correct the text orientation. From there, you can present the corrected image to your OCR engine (and ideally obtain higher OCR accuracy).

Learning Objectives
In this tutorial, you will learn:

The concept of orientation and script detection (OSD)
How to detect text script (i.e., writing system) with Tesseract
How to detect text orientation using Tesseract
How to automatically correct text orientation with OpenCV
Configuring your development environment
To follow this guide, you need to have the OpenCV library installed on your system.

Luckily, OpenCV is pip-installable:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
$ pip install opencv-contrib-python
If you need help configuring your development environment for OpenCV, I highly recommend that you read my pip install OpenCV guide — it will have you up and running in a matter of minutes.

Having problems configuring your development environment?

Figure 1: Having trouble configuring your dev environment? Want access to pre-configured Jupyter Notebooks running on Google Colab? Be sure to join PyImageSearch University — you’ll be up and running with this tutorial in a matter of minutes.
All that said, are you:

Short on time?
Learning on your employer’s administratively locked system?
Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?
Ready to run the code right now on your Windows, macOS, or Linux system?
Then join PyImageSearch University today!

Gain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.

And best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!

What Is Orientation and Script Detection?
Before we automatically detect and correct text orientation with Tesseract, we first need to discuss the concept of orientation and script detection (OSD). Tesseract has several different modes that you can use when automatically detecting and OCR’ing text. Some of these modes perform a full-blown OCR of the input image, while others output meta-data such as text information, orientation, etc. (i.e., your OSD modes). Tesseract’s OSD mode is going to give you two output values:

Text orientation: The estimated rotation angle (in degrees) of the text in the input image.
Script: The predicted “writing system” of the text in the image.
Figure 2 shows an example of varying text orientations. When in OSD mode, Tesseract will detect these orientations and tell us how to correct the orientation.


Figure 2. In OSD mode, Tesseract can detect text orientation and script type. From there, we can rotate the text back to 0° with OpenCV.
A writing system is a visual method of communicating information, but unlike speech, a writing system also includes the concept of “storage” and “knowledge transfer.”

When we put pen to paper, the characters we utilize are part of a script/writing system. These characters can be read by us and others, thereby imparting and transferring knowledge from the writer. Additionally, this knowledge is “stored” on the paper, meaning that if we were to die, the knowledge left on that paper could be imparted to others who could read our script/writing system.

Figure 2 also provides examples of various scripts and writing systems, including Latin (the script used in English and other languages) and Abjad (the script for Hebrew amid other languages). When placed in OSD mode, Tesseract automatically detects the text’s writing system in the input image.

If you’re new to the concept of scripts/writing systems, I would strongly recommend reading Wikipedia’s excellent article on the topic. It’s a great read which covers the history of writing systems and how they’ve evolved.

Detecting and Correcting Text Orientation with Tesseract
Now that we understand OSD’s basics, let’s move on to detecting and correcting text orientation with Tesseract. We’ll start with a quick review of our project directory structure. From there, I’ll show you how to implement text orientation correction. We’ll wrap up this tutorial with a discussion of our results.

Project Structure
Let’s dive into the directory structure for this project:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
|-- images
|   |-- normal.png
|   |-- rotated_180.png
|   |-- rotated_90_clockwise.png
|   |-- rotated_90_counter_clockwise.png
|   |-- rotated_90_counter_clockwise_hebrew.png
|-- detect_orientation.py
All the code to detect and correct text orientation is contained within the detect_orientation.py Python script and implemented in less than 35 lines of code, including comments. We’ll test the code using a selection of images/ included in the project folder.

Implementing Our Text Orientation and Correction Script
Let’s get started implementing our text orientation corrector with Tesseract and OpenCV.

Open a new file, name it detect_orientation.py, and insert the following code:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
# import the necessary packages
from pytesseract import Output
import pytesseract
import argparse
import imutils
import cv2
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,
	help="path to input image to be OCR'd")
args = vars(ap.parse_args())
An import you might not recognize at first is PyTesseract’s Output class (https://github.com/madmaze/pytesseract). This class simply specifies four datatypes including DICT which we will take advantage of.

Our lone command line argument is our input --image to be OCR’d. Let’s load the input now:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
# load the input image, convert it from BGR to RGB channel ordering,
# and use Tesseract to determine the text orientation
image = cv2.imread(args["image"])
rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
results = pytesseract.image_to_osd(rgb, output_type=Output.DICT)
# display the orientation information
print("[INFO] detected orientation: {}".format(
	results["orientation"]))
print("[INFO] rotate by {} degrees to correct".format(
	results["rotate"]))
print("[INFO] detected script: {}".format(results["script"]))
Lines 16 and 17 load our input --image and swap color channels so that it is compatible with Tesseract.

From there, we apply orientation and script detection (OSD) to the rgb image while specifying our output_type=Output.DICT (Line 18). We then display the orientation and script information in the terminal (contained in the results dictionary) including:

The current orientation
How many degrees to rotate the image to correct its orientation
The type of script detected, such as Latin or Arabic
Given this information, next, we’ll correct the text orientation:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
# rotate the image to correct the orientation
rotated = imutils.rotate_bound(image, angle=results["rotate"])
# show the original image and output image after orientation
# correction
cv2.imshow("Original", image)
cv2.imshow("Output", rotated)
cv2.waitKey(0)
Using my imutils rotate_bound method (http://pyimg.co/vebvn), we rotate the image ensuring that the entire image stays fully visible in the results (Line 28). Had we used OpenCV’s generic cv2.rotate method, the corners of the image would have been cut off. Finally, we display both the original and rotated images until a key is pressed (Lines 32-34).

Text Orientation and Correction Results
We are now ready to apply text OSD! Open a terminal and execute the following command:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
$ python detect_orientation.py --image images/normal.png
[INFO] detected orientation: 0
[INFO] rotate by 0 degrees to correct
[INFO] detected script: Latin
Figure 3 displays the results of our script and orientation detection. Notice that the input image has not been rotated, implying the orientation is 0°. No rotation correction is required. The script is then detected as “Latin.”


Figure 3. This screenshot from one of my autoencoder blog posts is already properly oriented. Thus, the input is the same as the output after correcting for text orientation with Tesseract.
Let’s try another image, this one with rotated text:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
$ python detect_orientation.py --image images/rotated_90_clockwise.png
[INFO] detected orientation: 90
[INFO] rotate by 270 degrees to correct
[INFO] detected script: Latin
Figure 4 shows the original input image, which contains rotated text. Using Tesseract in OSD mode, we can detect that the text in the input image has an orientation of 90° — we can correct this orientation by rotating the image 270° (i.e., −90°). And once again, the detected script is Latin.


Figure 4. As you can see, the input is not oriented in the way that we read side-to-side. Tesseract and OSD detect that the image is rotated 90°. From there, we use OpenCV to rotate the image 90° to counteract and re-orient the image.
We’ll wrap up this tutorial with one final example, this one of non-Latin text:

 → Launch Jupyter Notebook on Google Colab
Correcting Text Orientation with Tesseract and Python
$ python detect_orientation.py \
    --image images/rotated_90_counter_clockwise_hebrew.png
[INFO] detected orientation: 270
[INFO] rotate by 90 degrees to correct
[INFO] detected script: Hebrew
Figure 5 shows our input text image. We then detect the script (Hebrew) and correct its orientation by rotating the text 90°.


Figure 5. Tesseract can detect that this Hebrew input image is rotated 270° with OSD. We use OpenCV to rotate the image by 90° to correct this orientation problem.
As you can see, Tesseract makes text OSD easy!

What's next? I recommend PyImageSearch University.

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled
I strongly believe that if you had the right teacher you could master computer vision and deep learning.

Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?

That’s not the case.

All you need to master computer vision and deep learning is for someone to explain things to you in simple, intuitive terms. And that’s exactly what I do. My mission is to change education and how complex Artificial Intelligence topics are taught.

If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to successfully and confidently apply computer vision to your work, research, and projects. Join me in computer vision mastery.

Inside PyImageSearch University you'll find:

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.
CLICK HERE TO JOIN PYIMAGESEARCH UNIVERSITY

Summary
In this tutorial, you learned how to perform automatic text orientation detection and correction using Tesseract’s orientation and script detection (OSD) mode.

The OSD mode provides us with meta-data of the text in the image, including both estimated text orientation and script/writing system detection. The text orientation refers to the angle (in degrees) of the text in the image. When performing OCR, we can obtain higher accuracy by correcting for the text orientation. Script detection, on the other hand, refers to the writing system of the text, which could be Latin, Hanzi, Arabic, Hebrew, etc.

Want free GPU credits to train models?
We used Jarvislabs.ai, a GPU cloud, for all the experiments.
We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit here.
In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.

To overcome this problem, we use GPUs. The problem is these GPUs are expensive and become outdated quickly.

GPUs are great because they take your Neural Network and train it quickly. The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally. Cloud GPUs let you use a GPU and only pay for the time you are running the GPU. It’s a brilliant idea that saves you money.

JarvisLabs provides the best-in-class GPUs, and PyImageSearch University students get between 10 - 50 hours on a world-class GPU (time depends on the specific GPU you select).

This gives you a chance to test-drive a monstrously powerful GPU on any of our tutorials in a jiffy. So join PyImageSearch University today and try for yourself.

CLICK HERE TO GET JARVISLABS CREDITS NOW

To download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!


Download the Source Code and FREE 17-page Resource Guide
Enter your email address below to get a .zip of the code and a FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning. Inside you'll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!

Your email address
DOWNLOAD THE CODE!

About the Author
Hi there, I’m Adrian Rosebrock, PhD. All too often I see developers, students, and researchers wasting their time, studying the wrong things, and generally struggling to get started with Computer Vision, Deep Learning, and OpenCV. I created this website to show you what I believe is the best possible way to get your start.

Reader Interactions
Previous Article:

Torch Hub Series #6: Image Segmentation
Next Article:

Correcting Text Orientation with Tesseract and Python
Comment section
Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.

At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.

Instead, my goal is to do the most good for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.

If you need help learning computer vision and deep learning, I suggest you refer to my full catalog of books and courses — they have helped tens of thousands of developers, students, and researchers just like yourself learn Computer Vision, Deep Learning, and OpenCV.

Click here to browse my full catalog.

Primary Sidebar
PyImageSearch University — NOW ENROLLING!

You can master Computer Vision, Deep Learning, and OpenCV

Course information:
30+ total classes • 39h 44m video • Last updated: 12/2021
★★★★★
4.84 (128 Ratings) • 3,000+ Students Enrolled

✓ 30+ courses on essential computer vision, deep learning, and OpenCV topics
✓ 30+ Certificates of Completion
✓ 39h 44m on-demand video
✓ Brand new courses released every month, ensuring you can keep up with state-of-the-art techniques
✓ Pre-configured Jupyter Notebooks in Google Colab
✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)
✓ Access to centralized code repos for all 500+ tutorials on PyImageSearch
✓ Easy one-click downloads for code, datasets, pre-trained models, etc.
✓ Access on mobile, laptop, desktop, etc.

JOIN NOW
Picked For You

OCR Passports with OpenCV and Tesseract

Tesseract Page Segmentation Modes (PSMs) Explained: How to Improve Your OCR Accuracy

OCR’ing Business Cards

Automatically OCR’ing Receipts and Scans

Using Machine Learning to Denoise Images for Better OCR Accuracy
Similar articles
TUTORIALS
Basic motion detection and tracking with Python and OpenCV
May 25, 2015
DEEP LEARNINGDL4CV
Setting up Ubuntu 16.04 + CUDA + GPU for deep learning with Python
September 27, 2017
OPTICAL CHARACTER RECOGNITION (OCR)TUTORIALS
Tesseract Page Segmentation Modes (PSMs) Explained: How to Improve Your OCR Accuracy
November 15, 2021
